{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Developing Generative AI Using Python: A Step-by-Step Guide\n",
        "\n"
      ],
      "metadata": {
        "id": "ROYyQeil6DER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before that, a short example to see what it means to \"generate\" something out if something"
      ],
      "metadata": {
        "id": "-J4tnFPO6ynz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example shows how to generate a random 10 word sentence\n",
        " out of a sample sentence"
      ],
      "metadata": {
        "id": "Sh3-nsU569Pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Markov Chain: sequence generator which uses historic data to generate new sequence"
      ],
      "metadata": {
        "id": "N8j9k_wI8Tsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class MarkovChain:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "        self.states = {}\n",
        "\n",
        "        # Split the text into words\n",
        "        words = text.split()\n",
        "\n",
        "        # Create a dictionary of states\n",
        "        for i in range(len(words) - 1):\n",
        "            state = words[i]\n",
        "            next_state = words[i + 1]\n",
        "\n",
        "            if state not in self.states:\n",
        "                self.states[state] = []\n",
        "\n",
        "            self.states[state].append(next_state)\n",
        "\n",
        "    def generate(self, length):\n",
        "        # Choose a random starting state\n",
        "        state = random.choice(list(self.states.keys()))\n",
        "\n",
        "        # Generate a sequence of words\n",
        "        sentence = []\n",
        "        for i in range(length):\n",
        "            next_state = random.choice(self.states[state])\n",
        "            sentence.append(next_state)\n",
        "            state = next_state\n",
        "\n",
        "        # Return the generated sentence\n",
        "        return \" \".join(sentence)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a Markov chain\n",
        "    text = \"This is a sample text. It is used to train the Markov chain.\"\n",
        "    markov_chain = MarkovChain(text)\n",
        "\n",
        "    # Generate a sentence\n",
        "    sentence = markov_chain.generate(10)\n",
        "\n",
        "    # Print the generated sentence\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27DAG06x6c0_",
        "outputId": "d7f9a794-bb23-4eae-ac21-d757374816b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample text. It is a sample text. It is a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using RNNs to perform the same task"
      ],
      "metadata": {
        "id": "eEOPGIVW_Hjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install tensorflow"
      ],
      "metadata": {
        "id": "3L52veCY6Zy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "class RNNTextGenerator:\n",
        "    def __init__(self, text, seq_length=5):\n",
        "        self.text = text\n",
        "        self.seq_length = seq_length\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.tokenizer.fit_on_texts([text])\n",
        "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
        "\n",
        "        # Prepare the sequences\n",
        "        self.sequences = self.create_sequences()\n",
        "\n",
        "        # Build the model\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def create_sequences(self):\n",
        "        words = self.tokenizer.texts_to_sequences([self.text])[0]\n",
        "        sequences = []\n",
        "        for i in range(self.seq_length, len(words)):\n",
        "            seq = words[i - self.seq_length:i + 1]\n",
        "            sequences.append(seq)\n",
        "        return np.array(sequences)\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(self.vocab_size, 50, input_length=self.seq_length))\n",
        "        model.add(LSTM(100, return_sequences=True))\n",
        "        model.add(LSTM(100))\n",
        "        model.add(Dense(self.vocab_size, activation='softmax'))\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def train(self, epochs=100):\n",
        "        x, y = self.sequences[:, :-1], self.sequences[:, -1]\n",
        "        y = tf.keras.utils.to_categorical(y, num_classes=self.vocab_size)\n",
        "\n",
        "        self.model.fit(x, y, epochs=epochs, verbose=2)\n",
        "\n",
        "    def generate(self, seed_text, length=10):\n",
        "        result = []\n",
        "        input_text = seed_text.split()\n",
        "        for _ in range(length):\n",
        "            encoded = self.tokenizer.texts_to_sequences([input_text[-self.seq_length:]])[0]\n",
        "            encoded = pad_sequences([encoded], maxlen=self.seq_length, truncating='pre')\n",
        "\n",
        "            predicted = np.argmax(self.model.predict(encoded, verbose=0), axis=-1)\n",
        "            output_word = ''\n",
        "            for word, index in self.tokenizer.word_index.items():\n",
        "                if index == predicted:\n",
        "                    output_word = word\n",
        "                    break\n",
        "\n",
        "            input_text.append(output_word)\n",
        "            result.append(output_word)\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create RNN text generator\n",
        "    text = \"This is a sample text. It is used to train the Markov chain.\"\n",
        "    rnn_generator = RNNTextGenerator(text)\n",
        "\n",
        "    # Train the model\n",
        "    rnn_generator.train(epochs=100)\n",
        "\n",
        "    # Generate a sentence\n",
        "    seed_text = \"This is\"\n",
        "    sentence = rnn_generator.generate(seed_text, length=10)\n",
        "\n",
        "    # Print the generated sentence\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UoEVV5r_RUz",
        "outputId": "8f172d75-adc5-4c18-f907-ba46df06233f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 4s - loss: 2.5645 - accuracy: 0.1250 - 4s/epoch - 4s/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 2.5590 - accuracy: 0.5000 - 14ms/epoch - 14ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 2.5534 - accuracy: 0.5000 - 13ms/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 2.5473 - accuracy: 0.5000 - 14ms/epoch - 14ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 2.5407 - accuracy: 0.6250 - 14ms/epoch - 14ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 2.5333 - accuracy: 0.6250 - 17ms/epoch - 17ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 2.5249 - accuracy: 0.6250 - 17ms/epoch - 17ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 2.5155 - accuracy: 0.6250 - 14ms/epoch - 14ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 2.5046 - accuracy: 0.6250 - 15ms/epoch - 15ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 2.4922 - accuracy: 0.6250 - 15ms/epoch - 15ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 2.4779 - accuracy: 0.6250 - 14ms/epoch - 14ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 2.4614 - accuracy: 0.6250 - 19ms/epoch - 19ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 2.4423 - accuracy: 0.6250 - 26ms/epoch - 26ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 2.4203 - accuracy: 0.6250 - 14ms/epoch - 14ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 2.3950 - accuracy: 0.6250 - 17ms/epoch - 17ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 2.3658 - accuracy: 0.5000 - 15ms/epoch - 15ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 2.3326 - accuracy: 0.5000 - 14ms/epoch - 14ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 2.2950 - accuracy: 0.5000 - 15ms/epoch - 15ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 2.2529 - accuracy: 0.5000 - 14ms/epoch - 14ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 2.2068 - accuracy: 0.3750 - 14ms/epoch - 14ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 2.1576 - accuracy: 0.5000 - 18ms/epoch - 18ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 2.1067 - accuracy: 0.5000 - 16ms/epoch - 16ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 2.0562 - accuracy: 0.5000 - 14ms/epoch - 14ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 2.0081 - accuracy: 0.5000 - 14ms/epoch - 14ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 1.9636 - accuracy: 0.3750 - 14ms/epoch - 14ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 1.9223 - accuracy: 0.3750 - 15ms/epoch - 15ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 1.8825 - accuracy: 0.3750 - 17ms/epoch - 17ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 1.8420 - accuracy: 0.3750 - 17ms/epoch - 17ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 1.7988 - accuracy: 0.3750 - 16ms/epoch - 16ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 1.7511 - accuracy: 0.5000 - 14ms/epoch - 14ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 1.6979 - accuracy: 0.6250 - 15ms/epoch - 15ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 1.6389 - accuracy: 0.5000 - 16ms/epoch - 16ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 1.5748 - accuracy: 0.5000 - 14ms/epoch - 14ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 1.5075 - accuracy: 0.5000 - 21ms/epoch - 21ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 1.4393 - accuracy: 0.5000 - 15ms/epoch - 15ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 1.3721 - accuracy: 0.5000 - 16ms/epoch - 16ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 1.3073 - accuracy: 0.5000 - 15ms/epoch - 15ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 1.2459 - accuracy: 0.6250 - 15ms/epoch - 15ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 1.1882 - accuracy: 0.5000 - 15ms/epoch - 15ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 1.1327 - accuracy: 0.6250 - 15ms/epoch - 15ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 1.0764 - accuracy: 0.7500 - 17ms/epoch - 17ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 1.0166 - accuracy: 0.8750 - 21ms/epoch - 21ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.9530 - accuracy: 0.8750 - 15ms/epoch - 15ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.8874 - accuracy: 0.8750 - 15ms/epoch - 15ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.8216 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.7578 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.6973 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.6400 - accuracy: 1.0000 - 18ms/epoch - 18ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.5848 - accuracy: 1.0000 - 20ms/epoch - 20ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.5318 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.4809 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.4331 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.3905 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.3530 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.3192 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.2889 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.2615 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.2355 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.2110 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.1884 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.1680 - accuracy: 1.0000 - 19ms/epoch - 19ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.1497 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.1335 - accuracy: 1.0000 - 21ms/epoch - 21ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.1194 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.1072 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.0967 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.0876 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.0798 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.0730 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.0672 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.0621 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.0577 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.0537 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.0503 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.0472 - accuracy: 1.0000 - 18ms/epoch - 18ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.0445 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.0420 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.0399 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.0379 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.0362 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.0346 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.0331 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.0318 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.0305 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.0294 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.0283 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.0273 - accuracy: 1.0000 - 19ms/epoch - 19ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.0263 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.0254 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.0246 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.0238 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.0230 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.0223 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.0216 - accuracy: 1.0000 - 18ms/epoch - 18ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.0210 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.0204 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.0198 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.0193 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.0188 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.0183 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "it it it used the train the the chain chain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It still doesnt make any sense, we need something better and more \"contextual\"\n",
        "*   The quality of the generated text by Markov Chains and simple RNNs may often lack coherence, especially with small datasets or limited training epochs. While these models can generate sequences, their ability to capture the intricacies of language and context is limited compared to more advanced models.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "54xqsC6z_yrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "class LSTMTextGenerator:\n",
        "    def __init__(self, text, seq_length=5):\n",
        "        self.text = text\n",
        "        self.seq_length = seq_length\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.tokenizer.fit_on_texts([text])\n",
        "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
        "\n",
        "        # Prepare the sequences\n",
        "        self.sequences = self.create_sequences()\n",
        "\n",
        "        # Build the model\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def create_sequences(self):\n",
        "        words = self.tokenizer.texts_to_sequences([self.text])[0]\n",
        "        sequences = []\n",
        "        for i in range(self.seq_length, len(words)):\n",
        "            seq = words[i - self.seq_length:i + 1]\n",
        "            sequences.append(seq)\n",
        "        return np.array(sequences)\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(self.vocab_size, 50, input_length=self.seq_length))\n",
        "        model.add(LSTM(100, return_sequences=True))\n",
        "        model.add(LSTM(100))\n",
        "        model.add(Dense(self.vocab_size, activation='softmax'))\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def train(self, epochs=100):\n",
        "        x, y = self.sequences[:, :-1], self.sequences[:, -1]\n",
        "        y = tf.keras.utils.to_categorical(y, num_classes=self.vocab_size)\n",
        "\n",
        "        self.model.fit(x, y, epochs=epochs, verbose=2)\n",
        "\n",
        "    def generate(self, seed_text, length=10):\n",
        "        result = []\n",
        "        input_text = seed_text.split()\n",
        "        for _ in range(length):\n",
        "            encoded = self.tokenizer.texts_to_sequences([input_text[-self.seq_length:]])[0]\n",
        "            encoded = pad_sequences([encoded], maxlen=self.seq_length, truncating='pre')\n",
        "\n",
        "            predicted = np.argmax(self.model.predict(encoded, verbose=0), axis=-1)\n",
        "            output_word = ''\n",
        "            for word, index in self.tokenizer.word_index.items():\n",
        "                if index == predicted:\n",
        "                    output_word = word\n",
        "                    break\n",
        "\n",
        "            input_text.append(output_word)\n",
        "            result.append(output_word)\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create LSTM text generator\n",
        "    text = \"This is a sample text. It is used to train the Markov chain.\"\n",
        "    lstm_generator = LSTMTextGenerator(text)\n",
        "\n",
        "    # Train the model\n",
        "    lstm_generator.train(epochs=100)\n",
        "\n",
        "    # Generate a sentence\n",
        "    seed_text = \"This is\"\n",
        "    sentence = lstm_generator.generate(seed_text, length=10)\n",
        "\n",
        "    # Print the generated sentence\n",
        "    print(\"LSTM Generated Text: \", sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2tZDkjnRvWC",
        "outputId": "e383aa27-d86f-447b-dc77-474d60eeb9fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 4s - loss: 2.5650 - accuracy: 0.1250 - 4s/epoch - 4s/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 2.5592 - accuracy: 0.2500 - 14ms/epoch - 14ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 2.5534 - accuracy: 0.2500 - 13ms/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 2.5471 - accuracy: 0.3750 - 15ms/epoch - 15ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 2.5403 - accuracy: 0.3750 - 14ms/epoch - 14ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 2.5328 - accuracy: 0.3750 - 17ms/epoch - 17ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 2.5243 - accuracy: 0.3750 - 16ms/epoch - 16ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 2.5147 - accuracy: 0.3750 - 13ms/epoch - 13ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 2.5038 - accuracy: 0.3750 - 13ms/epoch - 13ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 2.4913 - accuracy: 0.3750 - 13ms/epoch - 13ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 2.4769 - accuracy: 0.2500 - 14ms/epoch - 14ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 2.4603 - accuracy: 0.2500 - 14ms/epoch - 14ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 2.4412 - accuracy: 0.2500 - 15ms/epoch - 15ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 2.4191 - accuracy: 0.2500 - 15ms/epoch - 15ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 2.3936 - accuracy: 0.2500 - 13ms/epoch - 13ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 2.3644 - accuracy: 0.2500 - 14ms/epoch - 14ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 2.3311 - accuracy: 0.1250 - 13ms/epoch - 13ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 2.2936 - accuracy: 0.1250 - 23ms/epoch - 23ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 2.2518 - accuracy: 0.1250 - 20ms/epoch - 20ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 2.2064 - accuracy: 0.1250 - 13ms/epoch - 13ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 2.1585 - accuracy: 0.1250 - 17ms/epoch - 17ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 2.1103 - accuracy: 0.1250 - 14ms/epoch - 14ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 2.0639 - accuracy: 0.1250 - 15ms/epoch - 15ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 2.0208 - accuracy: 0.1250 - 14ms/epoch - 14ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 1.9802 - accuracy: 0.2500 - 13ms/epoch - 13ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 1.9396 - accuracy: 0.2500 - 14ms/epoch - 14ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 1.8962 - accuracy: 0.2500 - 14ms/epoch - 14ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 1.8485 - accuracy: 0.2500 - 15ms/epoch - 15ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 1.7968 - accuracy: 0.3750 - 16ms/epoch - 16ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 1.7423 - accuracy: 0.5000 - 14ms/epoch - 14ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 1.6861 - accuracy: 0.6250 - 14ms/epoch - 14ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 1.6286 - accuracy: 0.6250 - 13ms/epoch - 13ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 1.5686 - accuracy: 0.5000 - 13ms/epoch - 13ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 1.5052 - accuracy: 0.5000 - 13ms/epoch - 13ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 1.4390 - accuracy: 0.5000 - 13ms/epoch - 13ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 1.3727 - accuracy: 0.6250 - 18ms/epoch - 18ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 1.3091 - accuracy: 0.7500 - 15ms/epoch - 15ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 1.2489 - accuracy: 0.6250 - 13ms/epoch - 13ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 1.1896 - accuracy: 0.6250 - 13ms/epoch - 13ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 1.1281 - accuracy: 0.6250 - 14ms/epoch - 14ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 1.0641 - accuracy: 0.6250 - 14ms/epoch - 14ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.9997 - accuracy: 0.8750 - 14ms/epoch - 14ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.9368 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.8761 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.8166 - accuracy: 0.8750 - 19ms/epoch - 19ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.7577 - accuracy: 0.8750 - 15ms/epoch - 15ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.6983 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.6389 - accuracy: 1.0000 - 20ms/epoch - 20ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.5829 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.5315 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.4831 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.4380 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.3976 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.3607 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.3241 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.2859 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.2481 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.2143 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.1832 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.1545 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.1310 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.1116 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.0950 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.0826 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.0733 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.0653 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.0584 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.0526 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.0477 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.0434 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.0396 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.0362 - accuracy: 1.0000 - 20ms/epoch - 20ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.0333 - accuracy: 1.0000 - 22ms/epoch - 22ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.0307 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.0285 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.0266 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.0250 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.0235 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.0222 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.0210 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.0200 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.0190 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.0182 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.0175 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.0168 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.0161 - accuracy: 1.0000 - 18ms/epoch - 18ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.0155 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.0150 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.0144 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.0139 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.0135 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.0131 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.0127 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.0123 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.0119 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.0116 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.0113 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.0110 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.0107 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.0105 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "LSTM Generated Text:  markov markov markov chain markov chain chain chain chain chain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using GRUs"
      ],
      "metadata": {
        "id": "36VFDGhTTVXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "class GRUTextGenerator:\n",
        "    def __init__(self, text, seq_length=5):\n",
        "        self.text = text\n",
        "        self.seq_length = seq_length\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.tokenizer.fit_on_texts([text])\n",
        "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
        "\n",
        "        # Prepare the sequences\n",
        "        self.sequences = self.create_sequences()\n",
        "\n",
        "        # Build the model\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def create_sequences(self):\n",
        "        words = self.tokenizer.texts_to_sequences([self.text])[0]\n",
        "        sequences = []\n",
        "        for i in range(self.seq_length, len(words)):\n",
        "            seq = words[i - self.seq_length:i + 1]\n",
        "            sequences.append(seq)\n",
        "        return np.array(sequences)\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(self.vocab_size, 50, input_length=self.seq_length))\n",
        "        model.add(GRU(100, return_sequences=True))\n",
        "        model.add(GRU(100))\n",
        "        model.add(Dense(self.vocab_size, activation='softmax'))\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def train(self, epochs=100):\n",
        "        x, y = self.sequences[:, :-1], self.sequences[:, -1]\n",
        "        y = tf.keras.utils.to_categorical(y, num_classes=self.vocab_size)\n",
        "\n",
        "        self.model.fit(x, y, epochs=epochs, verbose=2)\n",
        "\n",
        "    def generate(self, seed_text, length=10):\n",
        "        result = []\n",
        "        input_text = seed_text.split()\n",
        "        for _ in range(length):\n",
        "            encoded = self.tokenizer.texts_to_sequences([input_text[-self.seq_length:]])[0]\n",
        "            encoded = pad_sequences([encoded], maxlen=self.seq_length, truncating='pre')\n",
        "\n",
        "            predicted = np.argmax(self.model.predict(encoded, verbose=0), axis=-1)\n",
        "            output_word = ''\n",
        "            for word, index in self.tokenizer.word_index.items():\n",
        "                if index == predicted:\n",
        "                    output_word = word\n",
        "                    break\n",
        "\n",
        "            input_text.append(output_word)\n",
        "            result.append(output_word)\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create GRU text generator\n",
        "    text = \"This is a sample text. It is used to train the Markov chain.\"\n",
        "    gru_generator = GRUTextGenerator(text)\n",
        "\n",
        "    # Train the model\n",
        "    gru_generator.train(epochs=100)\n",
        "\n",
        "    # Generate a sentence\n",
        "    seed_text = \"This is\"\n",
        "    sentence = gru_generator.generate(seed_text, length=10)\n",
        "\n",
        "    # Print the generated sentence\n",
        "    print(\"GRU Generated Text: \", sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtBbzqmISV0O",
        "outputId": "064cb246-d522-4f89-ad31-a627961ce969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 4s - loss: 2.5665 - accuracy: 0.1250 - 4s/epoch - 4s/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 2.5534 - accuracy: 0.2500 - 12ms/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 2.5403 - accuracy: 0.2500 - 14ms/epoch - 14ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 2.5265 - accuracy: 0.2500 - 13ms/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 2.5118 - accuracy: 0.1250 - 14ms/epoch - 14ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 2.4956 - accuracy: 0.1250 - 13ms/epoch - 13ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 2.4778 - accuracy: 0.2500 - 15ms/epoch - 15ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 2.4578 - accuracy: 0.2500 - 13ms/epoch - 13ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 2.4355 - accuracy: 0.2500 - 13ms/epoch - 13ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 2.4104 - accuracy: 0.2500 - 14ms/epoch - 14ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 2.3824 - accuracy: 0.1250 - 14ms/epoch - 14ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 2.3512 - accuracy: 0.1250 - 13ms/epoch - 13ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 2.3167 - accuracy: 0.1250 - 13ms/epoch - 13ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 2.2792 - accuracy: 0.1250 - 17ms/epoch - 17ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 2.2389 - accuracy: 0.1250 - 14ms/epoch - 14ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 2.1966 - accuracy: 0.1250 - 14ms/epoch - 14ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 2.1533 - accuracy: 0.1250 - 13ms/epoch - 13ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 2.1097 - accuracy: 0.1250 - 14ms/epoch - 14ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 2.0663 - accuracy: 0.1250 - 12ms/epoch - 12ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 2.0225 - accuracy: 0.1250 - 14ms/epoch - 14ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 1.9769 - accuracy: 0.2500 - 13ms/epoch - 13ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 1.9287 - accuracy: 0.2500 - 15ms/epoch - 15ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 1.8783 - accuracy: 0.3750 - 16ms/epoch - 16ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 1.8268 - accuracy: 0.6250 - 15ms/epoch - 15ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 1.7754 - accuracy: 0.6250 - 16ms/epoch - 16ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 1.7234 - accuracy: 0.6250 - 14ms/epoch - 14ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 1.6674 - accuracy: 0.7500 - 14ms/epoch - 14ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 1.6042 - accuracy: 0.5000 - 14ms/epoch - 14ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 1.5329 - accuracy: 0.5000 - 13ms/epoch - 13ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 1.4554 - accuracy: 0.6250 - 17ms/epoch - 17ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 1.3750 - accuracy: 0.6250 - 13ms/epoch - 13ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 1.2941 - accuracy: 0.6250 - 13ms/epoch - 13ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 1.2138 - accuracy: 0.6250 - 13ms/epoch - 13ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 1.1338 - accuracy: 0.6250 - 15ms/epoch - 15ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 1.0537 - accuracy: 0.6250 - 13ms/epoch - 13ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.9742 - accuracy: 0.6250 - 19ms/epoch - 19ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.8963 - accuracy: 0.7500 - 14ms/epoch - 14ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.8213 - accuracy: 0.7500 - 13ms/epoch - 13ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.7510 - accuracy: 0.8750 - 13ms/epoch - 13ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.6863 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.6269 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.5713 - accuracy: 1.0000 - 21ms/epoch - 21ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.5185 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.4679 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.4198 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.3742 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.3306 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.2891 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.2501 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.2148 - accuracy: 1.0000 - 23ms/epoch - 23ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.1835 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.1562 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.1325 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.1124 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.0957 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.0822 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.0712 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.0622 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.0548 - accuracy: 1.0000 - 18ms/epoch - 18ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.0486 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.0433 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.0389 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.0351 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.0319 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.0291 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.0268 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.0248 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.0230 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.0214 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.0201 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.0188 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.0177 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.0167 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.0158 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.0150 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.0143 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.0136 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.0130 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.0125 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.0119 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.0115 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.0110 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.0106 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.0103 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.0099 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.0096 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.0093 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.0091 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.0088 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.0086 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.0084 - accuracy: 1.0000 - 18ms/epoch - 18ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.0082 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.0080 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.0078 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.0076 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.0075 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.0073 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.0072 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.0070 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.0069 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "GRU Generated Text:  markov markov markov markov markov it it is to train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variational Autoencoders (VAEs)"
      ],
      "metadata": {
        "id": "tGU6Z2WlUtCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Lambda, Embedding, RepeatVector, TimeDistributed, Flatten\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "class VAETextGenerator:\n",
        "    def __init__(self, text, seq_length=5, latent_dim=2):\n",
        "        self.text = text\n",
        "        self.seq_length = seq_length\n",
        "        self.latent_dim = latent_dim\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.tokenizer.fit_on_texts([text])\n",
        "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
        "\n",
        "        # Prepare the sequences\n",
        "        self.sequences = self.create_sequences()\n",
        "\n",
        "        # Build the VAE model\n",
        "        self.encoder, self.decoder, self.vae = self.build_model()\n",
        "\n",
        "    def create_sequences(self):\n",
        "        words = self.tokenizer.texts_to_sequences([self.text])[0]\n",
        "        sequences = []\n",
        "        for i in range(self.seq_length, len(words)):\n",
        "            seq = words[i - self.seq_length:i]\n",
        "            sequences.append(seq)\n",
        "        return np.array(sequences)\n",
        "\n",
        "    def sampling(self, args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        dim = K.int_shape(z_mean)[1]\n",
        "        epsilon = K.random_normal(shape=(batch, dim))\n",
        "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    def build_model(self):\n",
        "        # Encoder\n",
        "        inputs = Input(shape=(self.seq_length,))\n",
        "        x = Embedding(self.vocab_size, 50, input_length=self.seq_length)(inputs)\n",
        "        x = LSTM(100)(x)\n",
        "        z_mean = Dense(self.latent_dim)(x)\n",
        "        z_log_var = Dense(self.latent_dim)(x)\n",
        "        z = Lambda(self.sampling, output_shape=(self.latent_dim,))([z_mean, z_log_var])\n",
        "        encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "        # Decoder\n",
        "        latent_inputs = Input(shape=(self.latent_dim,))\n",
        "        x = Dense(100)(latent_inputs)\n",
        "        x = RepeatVector(self.seq_length)(x)\n",
        "        x = LSTM(100, return_sequences=True)(x)\n",
        "        outputs = TimeDistributed(Dense(self.vocab_size, activation='softmax'))(x)\n",
        "        decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "\n",
        "        # VAE\n",
        "        outputs = decoder(encoder(inputs)[2])\n",
        "        vae = Model(inputs, outputs, name='vae')\n",
        "\n",
        "        # VAE loss\n",
        "        def vae_loss(inputs, outputs):\n",
        "            reconstruction_loss = tf.keras.losses.sparse_categorical_crossentropy(inputs, outputs)\n",
        "            reconstruction_loss = K.sum(reconstruction_loss, axis=1)\n",
        "            kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "            kl_loss = K.sum(kl_loss, axis=-1)\n",
        "            kl_loss *= -0.5\n",
        "            return K.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "        vae.add_loss(vae_loss(inputs, outputs))\n",
        "        vae.compile(optimizer='adam')\n",
        "\n",
        "        return encoder, decoder, vae\n",
        "\n",
        "    def train(self, epochs=10):\n",
        "        x = pad_sequences(self.sequences, maxlen=self.seq_length)\n",
        "        self.vae.fit(x, epochs=epochs, batch_size=32, verbose=2)\n",
        "\n",
        "    def generate(self, seed_text, length=10):\n",
        "        encoded_seed = self.tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        encoded_seed = pad_sequences([encoded_seed], maxlen=self.seq_length, truncating='pre')\n",
        "        z_mean, _, _ = self.encoder.predict(encoded_seed)\n",
        "        generated = self.decoder.predict(z_mean)\n",
        "        generated_text = [self.tokenizer.index_word.get(np.argmax(word), '') for word in generated[0]]\n",
        "        return ' '.join(generated_text[:length])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_text = \"This is a sample text. It is used to train the Markov chain.\"\n",
        "    large_text = \" \".join([base_text] * 1000)\n",
        "\n",
        "    vae_generator = VAETextGenerator(large_text)\n",
        "    vae_generator.train(epochs=10)\n",
        "\n",
        "    seed_text = \"This is\"\n",
        "    sentence = vae_generator.generate(seed_text, length=10)\n",
        "    print(\"VAE Generated Text: \", sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXqjG0tnWtIC",
        "outputId": "ed187778-67f7-4bba-ebce-0b23ddfeab84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "407/407 - 13s - loss: 8.0805 - 13s/epoch - 31ms/step\n",
            "Epoch 2/10\n",
            "407/407 - 5s - loss: 4.6485 - 5s/epoch - 13ms/step\n",
            "Epoch 3/10\n",
            "407/407 - 6s - loss: 4.1526 - 6s/epoch - 15ms/step\n",
            "Epoch 4/10\n",
            "407/407 - 6s - loss: 4.0109 - 6s/epoch - 15ms/step\n",
            "Epoch 5/10\n",
            "407/407 - 5s - loss: 3.8812 - 5s/epoch - 13ms/step\n",
            "Epoch 6/10\n",
            "407/407 - 7s - loss: 3.7502 - 7s/epoch - 16ms/step\n",
            "Epoch 7/10\n",
            "407/407 - 5s - loss: 3.7159 - 5s/epoch - 13ms/step\n",
            "Epoch 8/10\n",
            "407/407 - 7s - loss: 3.7334 - 7s/epoch - 16ms/step\n",
            "Epoch 9/10\n",
            "407/407 - 5s - loss: 3.6523 - 5s/epoch - 13ms/step\n",
            "Epoch 10/10\n",
            "407/407 - 7s - loss: 3.5987 - 7s/epoch - 16ms/step\n",
            "1/1 [==============================] - 0s 450ms/step\n",
            "1/1 [==============================] - 0s 416ms/step\n",
            "VAE Generated Text:  markov chain this is a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GANs"
      ],
      "metadata": {
        "id": "6dAnrOC8XYWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Reshape, Flatten, TimeDistributed, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "class GANTextGenerator:\n",
        "    def __init__(self, text, seq_length=5):\n",
        "        self.text = text\n",
        "        self.seq_length = seq_length\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.tokenizer.fit_on_texts([text])\n",
        "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
        "\n",
        "        # Prepare the sequences\n",
        "        self.sequences = self.create_sequences()\n",
        "\n",
        "        # Build the GAN model\n",
        "        self.generator = self.build_generator()\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.gan = self.build_gan()\n",
        "\n",
        "    def create_sequences(self):\n",
        "        words = self.tokenizer.texts_to_sequences([self.text])[0]\n",
        "        sequences = []\n",
        "        for i in range(self.seq_length, len(words)):\n",
        "            seq = words[i - self.seq_length:i]\n",
        "            sequences.append(seq)\n",
        "        return np.array(sequences)\n",
        "\n",
        "    def build_generator(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(self.seq_length * 256, input_dim=self.seq_length, activation='relu'))\n",
        "        model.add(Reshape((self.seq_length, 256)))\n",
        "        model.add(LSTM(100, return_sequences=True))\n",
        "        model.add(TimeDistributed(Dense(self.vocab_size, activation='softmax')))\n",
        "        return model\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(100, input_shape=(self.seq_length, self.vocab_size), return_sequences=True))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_gan(self):\n",
        "        self.discriminator.trainable = False\n",
        "        model = Sequential()\n",
        "        model.add(self.generator)\n",
        "        model.add(self.discriminator)\n",
        "        model.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
        "        return model\n",
        "\n",
        "    def train(self, epochs=1000, batch_size=32):\n",
        "        half_batch = int(batch_size / 2)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            idx = np.random.randint(0, self.sequences.shape[0], half_batch)\n",
        "            real_seqs = self.sequences[idx]\n",
        "            real_seqs = pad_sequences(real_seqs, maxlen=self.seq_length)\n",
        "            real_seqs = tf.keras.utils.to_categorical(real_seqs, num_classes=self.vocab_size)\n",
        "\n",
        "            noise = np.random.normal(0, 1, (half_batch, self.seq_length))\n",
        "            gen_seqs = self.generator.predict(noise)\n",
        "\n",
        "            d_loss_real = self.discriminator.train_on_batch(real_seqs, np.ones((half_batch, 1)))\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, np.zeros((half_batch, 1)))\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.seq_length))\n",
        "            valid_y = np.array([1] * batch_size)\n",
        "            g_loss = self.gan.train_on_batch(noise, valid_y)\n",
        "\n",
        "            print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
        "\n",
        "    def generate(self, length=10):\n",
        "        noise = np.random.normal(0, 1, (1, self.seq_length))\n",
        "        generated = self.generator.predict(noise)\n",
        "        generated_text = [self.tokenizer.index_word.get(np.argmax(word), '') for word in generated[0]]\n",
        "        return ' '.join(generated_text[:length])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_text = \"This is a sample text. It is used to train the Markov chain.\"\n",
        "    large_text = \" \".join([base_text] * 1000)\n",
        "\n",
        "    gan_generator = GANTextGenerator(large_text)\n",
        "    gan_generator.train(epochs=1000, batch_size=32)\n",
        "\n",
        "    sentence = gan_generator.generate(length=10)\n",
        "    print(\"GAN Generated Text: \", sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaaUDdM7X4lv",
        "outputId": "98da478f-5cd6-4362-f6f8-188c88b7c7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 578ms/step\n",
            "0 [D loss: 0.6987886726856232, acc.: 21.875] [G loss: 0.6818361878395081]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1 [D loss: 0.6937481462955475, acc.: 46.875] [G loss: 0.6800020933151245]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "2 [D loss: 0.6957064568996429, acc.: 34.375] [G loss: 0.6793464422225952]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "3 [D loss: 0.6914526224136353, acc.: 50.0] [G loss: 0.6789439916610718]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "4 [D loss: 0.6918761134147644, acc.: 46.875] [G loss: 0.6786562204360962]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "5 [D loss: 0.6893258392810822, acc.: 46.875] [G loss: 0.6788207292556763]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "6 [D loss: 0.6921645700931549, acc.: 43.75] [G loss: 0.6786749958992004]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "7 [D loss: 0.6922245621681213, acc.: 43.75] [G loss: 0.6782711744308472]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "8 [D loss: 0.6899897456169128, acc.: 50.0] [G loss: 0.6781689524650574]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "9 [D loss: 0.6852372586727142, acc.: 50.0] [G loss: 0.6779918074607849]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "10 [D loss: 0.693285346031189, acc.: 34.375] [G loss: 0.6774842739105225]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "11 [D loss: 0.6889465749263763, acc.: 46.875] [G loss: 0.6778182983398438]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "12 [D loss: 0.6910552382469177, acc.: 43.75] [G loss: 0.6766859292984009]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "13 [D loss: 0.6884288787841797, acc.: 46.875] [G loss: 0.6759834289550781]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "14 [D loss: 0.6875741481781006, acc.: 50.0] [G loss: 0.6760337352752686]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "15 [D loss: 0.6892706155776978, acc.: 43.75] [G loss: 0.6760069727897644]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "16 [D loss: 0.6889916062355042, acc.: 43.75] [G loss: 0.6745541095733643]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "17 [D loss: 0.692376583814621, acc.: 34.375] [G loss: 0.6749024391174316]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "18 [D loss: 0.6862752139568329, acc.: 50.0] [G loss: 0.6744238138198853]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "19 [D loss: 0.686473160982132, acc.: 50.0] [G loss: 0.674321174621582]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "20 [D loss: 0.6896846890449524, acc.: 50.0] [G loss: 0.672892689704895]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "21 [D loss: 0.6871612668037415, acc.: 50.0] [G loss: 0.673255205154419]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "22 [D loss: 0.6872075200080872, acc.: 50.0] [G loss: 0.6713451147079468]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "23 [D loss: 0.6860166788101196, acc.: 50.0] [G loss: 0.6713739037513733]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "24 [D loss: 0.689476728439331, acc.: 50.0] [G loss: 0.6683437824249268]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "25 [D loss: 0.6879425048828125, acc.: 50.0] [G loss: 0.6677063703536987]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "26 [D loss: 0.6908788979053497, acc.: 50.0] [G loss: 0.670005738735199]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "27 [D loss: 0.6886371970176697, acc.: 50.0] [G loss: 0.6698817610740662]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "28 [D loss: 0.6884400248527527, acc.: 50.0] [G loss: 0.6746923923492432]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "29 [D loss: 0.6863520741462708, acc.: 50.0] [G loss: 0.6849524974822998]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "30 [D loss: 0.6770714521408081, acc.: 50.0] [G loss: 0.6973795890808105]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "31 [D loss: 0.6726574301719666, acc.: 100.0] [G loss: 0.7143261432647705]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "32 [D loss: 0.6638546586036682, acc.: 100.0] [G loss: 0.7357298731803894]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "33 [D loss: 0.6500161588191986, acc.: 100.0] [G loss: 0.7603831887245178]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "34 [D loss: 0.6402589678764343, acc.: 100.0] [G loss: 0.7885494232177734]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "35 [D loss: 0.6291577219963074, acc.: 96.875] [G loss: 0.8192769289016724]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "36 [D loss: 0.6124871373176575, acc.: 100.0] [G loss: 0.8577776551246643]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "37 [D loss: 0.602234810590744, acc.: 96.875] [G loss: 0.8918034434318542]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "38 [D loss: 0.5851058959960938, acc.: 96.875] [G loss: 0.922006368637085]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "39 [D loss: 0.5877200365066528, acc.: 81.25] [G loss: 0.9541339874267578]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "40 [D loss: 0.5630788207054138, acc.: 93.75] [G loss: 0.9762958288192749]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "41 [D loss: 0.5482565015554428, acc.: 87.5] [G loss: 0.9632152318954468]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "42 [D loss: 0.5650644898414612, acc.: 84.375] [G loss: 0.9946874976158142]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "43 [D loss: 0.5879247784614563, acc.: 84.375] [G loss: 0.951501190662384]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "44 [D loss: 0.5855137705802917, acc.: 87.5] [G loss: 0.9392104148864746]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "45 [D loss: 0.5922872424125671, acc.: 84.375] [G loss: 0.8068181276321411]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "46 [D loss: 0.6069769561290741, acc.: 65.625] [G loss: 0.7623212337493896]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "47 [D loss: 0.6092618107795715, acc.: 68.75] [G loss: 0.7120674848556519]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "48 [D loss: 0.6423682570457458, acc.: 46.875] [G loss: 0.6422774791717529]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "49 [D loss: 0.671522468328476, acc.: 43.75] [G loss: 0.6158249974250793]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "50 [D loss: 0.6925202310085297, acc.: 28.125] [G loss: 0.5837301015853882]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "51 [D loss: 0.67006915807724, acc.: 46.875] [G loss: 0.5739721059799194]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "52 [D loss: 0.7605365514755249, acc.: 18.75] [G loss: 0.5687273144721985]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "53 [D loss: 0.7549478709697723, acc.: 21.875] [G loss: 0.5697591304779053]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "54 [D loss: 0.7248888611793518, acc.: 34.375] [G loss: 0.5764886140823364]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "55 [D loss: 0.7131303548812866, acc.: 34.375] [G loss: 0.5845690369606018]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "56 [D loss: 0.7228697240352631, acc.: 28.125] [G loss: 0.5967738032341003]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "57 [D loss: 0.691548079252243, acc.: 34.375] [G loss: 0.6010398864746094]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "58 [D loss: 0.7321523725986481, acc.: 34.375] [G loss: 0.6172835230827332]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "59 [D loss: 0.6992079019546509, acc.: 28.125] [G loss: 0.6211650371551514]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "60 [D loss: 0.7236328423023224, acc.: 25.0] [G loss: 0.6244466304779053]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "61 [D loss: 0.7187655866146088, acc.: 21.875] [G loss: 0.6356703042984009]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "62 [D loss: 0.714156448841095, acc.: 21.875] [G loss: 0.6398131251335144]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "63 [D loss: 0.687257319688797, acc.: 37.5] [G loss: 0.6402601003646851]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "64 [D loss: 0.7104926705360413, acc.: 28.125] [G loss: 0.6485638618469238]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "65 [D loss: 0.7091527879238129, acc.: 21.875] [G loss: 0.6521991491317749]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "66 [D loss: 0.6740139722824097, acc.: 43.75] [G loss: 0.6500550508499146]\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "67 [D loss: 0.6802089512348175, acc.: 43.75] [G loss: 0.6599868535995483]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "68 [D loss: 0.6816972494125366, acc.: 34.375] [G loss: 0.6609799861907959]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "69 [D loss: 0.6778974831104279, acc.: 40.625] [G loss: 0.6674036979675293]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "70 [D loss: 0.6865245699882507, acc.: 43.75] [G loss: 0.6721446514129639]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "71 [D loss: 0.6877061128616333, acc.: 43.75] [G loss: 0.6762453317642212]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "72 [D loss: 0.6867177486419678, acc.: 40.625] [G loss: 0.6751276254653931]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "73 [D loss: 0.6819073855876923, acc.: 34.375] [G loss: 0.6823152303695679]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "74 [D loss: 0.7096577286720276, acc.: 34.375] [G loss: 0.6807734966278076]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "75 [D loss: 0.6956936717033386, acc.: 34.375] [G loss: 0.6878118515014648]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "76 [D loss: 0.6817798018455505, acc.: 43.75] [G loss: 0.6914366483688354]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "77 [D loss: 0.664071649312973, acc.: 50.0] [G loss: 0.6919951438903809]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "78 [D loss: 0.6629000902175903, acc.: 50.0] [G loss: 0.6951989531517029]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "79 [D loss: 0.6751399338245392, acc.: 50.0] [G loss: 0.7006797790527344]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "80 [D loss: 0.6259629130363464, acc.: 65.625] [G loss: 0.7028535604476929]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "81 [D loss: 0.6564050018787384, acc.: 68.75] [G loss: 0.7062238454818726]\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "82 [D loss: 0.6418049931526184, acc.: 75.0] [G loss: 0.7086413502693176]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "83 [D loss: 0.6474149525165558, acc.: 71.875] [G loss: 0.7153573632240295]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "84 [D loss: 0.6779301166534424, acc.: 59.375] [G loss: 0.7166743278503418]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "85 [D loss: 0.6320752203464508, acc.: 78.125] [G loss: 0.7242797613143921]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "86 [D loss: 0.5868219286203384, acc.: 84.375] [G loss: 0.7275123000144958]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "87 [D loss: 0.6271774172782898, acc.: 90.625] [G loss: 0.7405452728271484]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "88 [D loss: 0.6091101765632629, acc.: 84.375] [G loss: 0.7528577446937561]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "89 [D loss: 0.5872341096401215, acc.: 90.625] [G loss: 0.7739511728286743]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "90 [D loss: 0.5948279798030853, acc.: 81.25] [G loss: 0.8075617551803589]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "91 [D loss: 0.5992293953895569, acc.: 71.875] [G loss: 0.8473560810089111]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "92 [D loss: 0.5964086651802063, acc.: 71.875] [G loss: 0.9032469987869263]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "93 [D loss: 0.5426931977272034, acc.: 84.375] [G loss: 0.9754239320755005]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "94 [D loss: 0.5050538331270218, acc.: 81.25] [G loss: 1.0667991638183594]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "95 [D loss: 0.4404844790697098, acc.: 87.5] [G loss: 1.1806055307388306]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "96 [D loss: 0.4604123532772064, acc.: 90.625] [G loss: 1.2948086261749268]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "97 [D loss: 0.3785117119550705, acc.: 87.5] [G loss: 1.413309097290039]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "98 [D loss: 0.3343205749988556, acc.: 87.5] [G loss: 1.5481497049331665]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "99 [D loss: 0.29048433154821396, acc.: 96.875] [G loss: 1.674997329711914]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "100 [D loss: 0.25345446169376373, acc.: 100.0] [G loss: 1.8174902200698853]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "101 [D loss: 0.27170033752918243, acc.: 96.875] [G loss: 1.912778377532959]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "102 [D loss: 0.20889516174793243, acc.: 100.0] [G loss: 1.9878431558609009]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "103 [D loss: 0.21973980218172073, acc.: 96.875] [G loss: 2.0759716033935547]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "104 [D loss: 0.15592941641807556, acc.: 100.0] [G loss: 2.050710439682007]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "105 [D loss: 0.2013552486896515, acc.: 96.875] [G loss: 1.9190335273742676]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "106 [D loss: 0.23507995903491974, acc.: 93.75] [G loss: 1.7105144262313843]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "107 [D loss: 0.21399620920419693, acc.: 96.875] [G loss: 1.6028825044631958]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "108 [D loss: 0.19546499103307724, acc.: 100.0] [G loss: 1.5185933113098145]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "109 [D loss: 0.2791409194469452, acc.: 93.75] [G loss: 1.711006999015808]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "110 [D loss: 0.3267211318016052, acc.: 90.625] [G loss: 1.6488168239593506]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "111 [D loss: 0.27216997742652893, acc.: 93.75] [G loss: 1.8162589073181152]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "112 [D loss: 0.20890577137470245, acc.: 96.875] [G loss: 1.9718233346939087]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "113 [D loss: 0.2526915594935417, acc.: 93.75] [G loss: 1.9662649631500244]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "114 [D loss: 0.25834934413433075, acc.: 90.625] [G loss: 1.9673733711242676]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "115 [D loss: 0.2711131274700165, acc.: 90.625] [G loss: 1.8570337295532227]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "116 [D loss: 0.21328957378864288, acc.: 93.75] [G loss: 1.7152540683746338]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "117 [D loss: 0.1909499242901802, acc.: 96.875] [G loss: 1.6594632863998413]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "118 [D loss: 0.20512238144874573, acc.: 93.75] [G loss: 1.6914435625076294]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "119 [D loss: 0.1517793834209442, acc.: 100.0] [G loss: 1.757190465927124]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "120 [D loss: 0.13358527049422264, acc.: 100.0] [G loss: 1.962667465209961]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "121 [D loss: 0.1607925221323967, acc.: 100.0] [G loss: 2.224412679672241]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "122 [D loss: 0.09801183268427849, acc.: 100.0] [G loss: 2.453294277191162]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "123 [D loss: 0.0848495326936245, acc.: 100.0] [G loss: 2.6624979972839355]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "124 [D loss: 0.05281395465135574, acc.: 100.0] [G loss: 2.905980110168457]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "125 [D loss: 0.06187899969518185, acc.: 100.0] [G loss: 2.978809118270874]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "126 [D loss: 0.05887715145945549, acc.: 100.0] [G loss: 3.0938425064086914]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "127 [D loss: 0.04850190505385399, acc.: 100.0] [G loss: 3.175889015197754]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "128 [D loss: 0.03959624655544758, acc.: 100.0] [G loss: 3.2278857231140137]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "129 [D loss: 0.04571335390210152, acc.: 100.0] [G loss: 3.301248550415039]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "130 [D loss: 0.0438255462795496, acc.: 100.0] [G loss: 3.444732189178467]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "131 [D loss: 0.049625616520643234, acc.: 100.0] [G loss: 3.428083658218384]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "132 [D loss: 0.051498010754585266, acc.: 100.0] [G loss: 3.335171937942505]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "133 [D loss: 0.047150325030088425, acc.: 100.0] [G loss: 3.440068244934082]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "134 [D loss: 0.045877480879426, acc.: 100.0] [G loss: 3.6011292934417725]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "135 [D loss: 0.05026206746697426, acc.: 100.0] [G loss: 3.6306025981903076]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "136 [D loss: 0.051592559553682804, acc.: 100.0] [G loss: 3.580810070037842]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "137 [D loss: 0.04265442118048668, acc.: 100.0] [G loss: 3.950543165206909]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "138 [D loss: 0.04027231317013502, acc.: 100.0] [G loss: 4.049394607543945]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "139 [D loss: 0.024847627617418766, acc.: 100.0] [G loss: 4.843499183654785]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "140 [D loss: 0.020692595280706882, acc.: 100.0] [G loss: 4.990144729614258]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "141 [D loss: 0.11311877192929387, acc.: 96.875] [G loss: 4.788460731506348]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "142 [D loss: 0.11669584549963474, acc.: 96.875] [G loss: 3.607578992843628]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "143 [D loss: 0.05504568852484226, acc.: 100.0] [G loss: 3.0536534786224365]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "144 [D loss: 0.11376449465751648, acc.: 93.75] [G loss: 2.9301254749298096]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "145 [D loss: 0.08350695669651031, acc.: 100.0] [G loss: 3.1793041229248047]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "146 [D loss: 0.03845703415572643, acc.: 100.0] [G loss: 3.958585739135742]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "147 [D loss: 0.04469972103834152, acc.: 100.0] [G loss: 4.156658172607422]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "148 [D loss: 0.0484162662178278, acc.: 100.0] [G loss: 3.926391839981079]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "149 [D loss: 0.07342171669006348, acc.: 100.0] [G loss: 3.2189908027648926]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "150 [D loss: 0.06071348860859871, acc.: 100.0] [G loss: 3.2569215297698975]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "151 [D loss: 0.04578899126499891, acc.: 100.0] [G loss: 3.9513940811157227]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "152 [D loss: 0.01544014597311616, acc.: 100.0] [G loss: 5.047552585601807]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "153 [D loss: 0.036055861972272396, acc.: 100.0] [G loss: 5.600897789001465]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "154 [D loss: 0.14234498422592878, acc.: 96.875] [G loss: 5.023612022399902]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "155 [D loss: 0.2835409939289093, acc.: 93.75] [G loss: 3.0564985275268555]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "156 [D loss: 0.24733403325080872, acc.: 96.875] [G loss: 2.6027252674102783]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "157 [D loss: 0.0383914546109736, acc.: 100.0] [G loss: 4.211615562438965]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "158 [D loss: 0.010573487728834152, acc.: 100.0] [G loss: 5.4305925369262695]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "159 [D loss: 0.0191530657466501, acc.: 100.0] [G loss: 6.172827243804932]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "160 [D loss: 0.19204249698668718, acc.: 96.875] [G loss: 5.706488609313965]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "161 [D loss: 0.07938017696142197, acc.: 96.875] [G loss: 3.8928070068359375]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "162 [D loss: 0.5630129277706146, acc.: 84.375] [G loss: 3.8220152854919434]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "163 [D loss: 0.7244223244488239, acc.: 84.375] [G loss: 4.870125770568848]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "164 [D loss: 0.39899514243006706, acc.: 90.625] [G loss: 4.702694892883301]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "165 [D loss: 0.2818723460659385, acc.: 87.5] [G loss: 4.235838890075684]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "166 [D loss: 0.06615545973181725, acc.: 96.875] [G loss: 4.372177600860596]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "167 [D loss: 0.034717336762696505, acc.: 100.0] [G loss: 4.286380767822266]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "168 [D loss: 0.03273879550397396, acc.: 100.0] [G loss: 4.378898620605469]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "169 [D loss: 0.006512123567517847, acc.: 100.0] [G loss: 4.574652671813965]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "170 [D loss: 0.01574630942195654, acc.: 100.0] [G loss: 4.832691192626953]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "171 [D loss: 0.009165694937109947, acc.: 100.0] [G loss: 5.091846942901611]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "172 [D loss: 0.01070677232928574, acc.: 100.0] [G loss: 5.28441047668457]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "173 [D loss: 0.008341305889189243, acc.: 100.0] [G loss: 5.569400787353516]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "174 [D loss: 0.0028609223663806915, acc.: 100.0] [G loss: 5.7493181228637695]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "175 [D loss: 0.008513317094184458, acc.: 100.0] [G loss: 5.946663856506348]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "176 [D loss: 0.005741564556956291, acc.: 100.0] [G loss: 6.142163276672363]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "177 [D loss: 0.002956150798127055, acc.: 100.0] [G loss: 6.380561828613281]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "178 [D loss: 0.0022558553609997034, acc.: 100.0] [G loss: 6.481692314147949]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "179 [D loss: 0.0029613019432872534, acc.: 100.0] [G loss: 6.6323957443237305]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "180 [D loss: 0.002914662822149694, acc.: 100.0] [G loss: 6.739156723022461]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "181 [D loss: 0.0023979460820555687, acc.: 100.0] [G loss: 6.842954635620117]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "182 [D loss: 0.003283374710008502, acc.: 100.0] [G loss: 6.890509605407715]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "183 [D loss: 0.0036959819844923913, acc.: 100.0] [G loss: 6.95600700378418]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "184 [D loss: 0.002249693206977099, acc.: 100.0] [G loss: 6.996744155883789]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "185 [D loss: 0.0021932453382760286, acc.: 100.0] [G loss: 7.061000823974609]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "186 [D loss: 0.0017349258996546268, acc.: 100.0] [G loss: 7.049349784851074]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "187 [D loss: 0.0018887969199568033, acc.: 100.0] [G loss: 7.068536758422852]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "188 [D loss: 0.0020822769729420543, acc.: 100.0] [G loss: 7.061654090881348]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "189 [D loss: 0.002212870283983648, acc.: 100.0] [G loss: 7.072348594665527]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "190 [D loss: 0.0018782360712066293, acc.: 100.0] [G loss: 7.05391788482666]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "191 [D loss: 0.000997632509097457, acc.: 100.0] [G loss: 7.019554138183594]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "192 [D loss: 0.0022916283924132586, acc.: 100.0] [G loss: 6.937563896179199]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "193 [D loss: 0.0013208913151174784, acc.: 100.0] [G loss: 6.976613998413086]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "194 [D loss: 0.0016649997560307384, acc.: 100.0] [G loss: 7.00056266784668]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "195 [D loss: 0.002048987487796694, acc.: 100.0] [G loss: 6.937130928039551]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "196 [D loss: 0.0020182287553325295, acc.: 100.0] [G loss: 6.822938442230225]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "197 [D loss: 0.001558011572342366, acc.: 100.0] [G loss: 6.739480972290039]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "198 [D loss: 0.0008597148698754609, acc.: 100.0] [G loss: 6.731484413146973]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "199 [D loss: 0.0026628326158970594, acc.: 100.0] [G loss: 6.490108013153076]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "200 [D loss: 0.002304492052644491, acc.: 100.0] [G loss: 6.377890586853027]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "201 [D loss: 0.0028668902814388275, acc.: 100.0] [G loss: 5.840387344360352]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "202 [D loss: 0.0035642466973513365, acc.: 100.0] [G loss: 5.7855424880981445]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "203 [D loss: 0.005099066998809576, acc.: 100.0] [G loss: 5.49232292175293]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "204 [D loss: 0.008443505736067891, acc.: 100.0] [G loss: 5.368181228637695]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "205 [D loss: 0.0076903641456738114, acc.: 100.0] [G loss: 4.566437721252441]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "206 [D loss: 0.014378395630046725, acc.: 100.0] [G loss: 4.366321563720703]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "207 [D loss: 0.023829124867916107, acc.: 100.0] [G loss: 4.064733505249023]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "208 [D loss: 0.03602466266602278, acc.: 100.0] [G loss: 3.820077657699585]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "209 [D loss: 0.053874516393989325, acc.: 100.0] [G loss: 4.365918159484863]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "210 [D loss: 0.04735677223652601, acc.: 100.0] [G loss: 4.206343173980713]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "211 [D loss: 0.10344056785106659, acc.: 100.0] [G loss: 4.015946388244629]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "212 [D loss: 0.15232674032449722, acc.: 96.875] [G loss: 3.979832172393799]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "213 [D loss: 0.22865404933691025, acc.: 93.75] [G loss: 4.238900184631348]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "214 [D loss: 0.2862275540828705, acc.: 96.875] [G loss: 4.218071937561035]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "215 [D loss: 0.09311990812420845, acc.: 100.0] [G loss: 4.708412170410156]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "216 [D loss: 0.2628113701939583, acc.: 96.875] [G loss: 5.44266414642334]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "217 [D loss: 0.35657179541885853, acc.: 93.75] [G loss: 5.557639122009277]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "218 [D loss: 0.5963070755824447, acc.: 87.5] [G loss: 4.674750328063965]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "219 [D loss: 0.23186930641531944, acc.: 93.75] [G loss: 3.717874050140381]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "220 [D loss: 0.7522086333483458, acc.: 78.125] [G loss: 3.157607316970825]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "221 [D loss: 0.050804135389626026, acc.: 100.0] [G loss: 3.264814615249634]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "222 [D loss: 0.0709853395819664, acc.: 96.875] [G loss: 3.9891183376312256]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "223 [D loss: 0.009051873115822673, acc.: 100.0] [G loss: 4.870574951171875]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "224 [D loss: 0.03143909783102572, acc.: 100.0] [G loss: 5.368466377258301]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "225 [D loss: 0.011225390015169978, acc.: 100.0] [G loss: 5.800739288330078]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "226 [D loss: 0.02212409279309213, acc.: 100.0] [G loss: 5.885289192199707]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "227 [D loss: 0.019366617780178785, acc.: 100.0] [G loss: 5.433896541595459]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "228 [D loss: 0.004046799964271486, acc.: 100.0] [G loss: 4.910850524902344]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "229 [D loss: 0.007078276947140694, acc.: 100.0] [G loss: 4.545764923095703]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "230 [D loss: 0.008864521747455001, acc.: 100.0] [G loss: 4.523187160491943]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "231 [D loss: 0.005882346071302891, acc.: 100.0] [G loss: 4.873091220855713]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "232 [D loss: 0.004471798427402973, acc.: 100.0] [G loss: 5.416934013366699]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "233 [D loss: 0.002688775595743209, acc.: 100.0] [G loss: 5.882412910461426]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "234 [D loss: 0.0018099474254995584, acc.: 100.0] [G loss: 6.360843658447266]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "235 [D loss: 0.0025690404581837356, acc.: 100.0] [G loss: 6.698270320892334]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "236 [D loss: 0.0024728442076593637, acc.: 100.0] [G loss: 6.937499046325684]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "237 [D loss: 0.003308231505798176, acc.: 100.0] [G loss: 7.115683555603027]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "238 [D loss: 0.0024887131294235587, acc.: 100.0] [G loss: 7.226649284362793]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "239 [D loss: 0.002206330798799172, acc.: 100.0] [G loss: 7.275826930999756]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "240 [D loss: 0.002386241510976106, acc.: 100.0] [G loss: 7.285068988800049]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "241 [D loss: 0.0036620981409214437, acc.: 100.0] [G loss: 7.240155220031738]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "242 [D loss: 0.0015535402344539762, acc.: 100.0] [G loss: 7.211369037628174]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "243 [D loss: 0.0006723325932398438, acc.: 100.0] [G loss: 7.211863994598389]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "244 [D loss: 0.0020324451033957303, acc.: 100.0] [G loss: 7.175776481628418]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "245 [D loss: 0.000895585078978911, acc.: 100.0] [G loss: 7.143535614013672]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "246 [D loss: 0.0014682285254821181, acc.: 100.0] [G loss: 7.156643867492676]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "247 [D loss: 0.0008016158826649189, acc.: 100.0] [G loss: 7.189699172973633]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "248 [D loss: 0.0005366800760384649, acc.: 100.0] [G loss: 7.200456619262695]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "249 [D loss: 0.0025027815718203783, acc.: 100.0] [G loss: 7.2090654373168945]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "250 [D loss: 0.0013897945755161345, acc.: 100.0] [G loss: 7.227506637573242]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "251 [D loss: 0.0021965352934785187, acc.: 100.0] [G loss: 7.2063798904418945]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "252 [D loss: 0.0018582340562716126, acc.: 100.0] [G loss: 7.156350612640381]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "253 [D loss: 0.00118087072041817, acc.: 100.0] [G loss: 7.132307052612305]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "254 [D loss: 0.0022687322343699634, acc.: 100.0] [G loss: 7.110705852508545]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "255 [D loss: 0.001219689118443057, acc.: 100.0] [G loss: 7.0996012687683105]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "256 [D loss: 0.0012893345847260207, acc.: 100.0] [G loss: 7.122976303100586]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "257 [D loss: 0.0012136186414863914, acc.: 100.0] [G loss: 7.101556777954102]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "258 [D loss: 0.0009130101534537971, acc.: 100.0] [G loss: 7.149969577789307]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "259 [D loss: 0.0007644786965101957, acc.: 100.0] [G loss: 7.188803672790527]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "260 [D loss: 0.0012788185267709196, acc.: 100.0] [G loss: 7.2488112449646]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "261 [D loss: 0.0006393686053343117, acc.: 100.0] [G loss: 7.2997283935546875]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "262 [D loss: 0.0020346591772977263, acc.: 100.0] [G loss: 7.347785949707031]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "263 [D loss: 0.0007401799375656992, acc.: 100.0] [G loss: 7.376591682434082]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "264 [D loss: 0.001821184268919751, acc.: 100.0] [G loss: 7.384182453155518]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "265 [D loss: 0.0004523439856711775, acc.: 100.0] [G loss: 7.398647308349609]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "266 [D loss: 0.0009497427381575108, acc.: 100.0] [G loss: 7.421783924102783]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "267 [D loss: 0.000968402047874406, acc.: 100.0] [G loss: 7.4464521408081055]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "268 [D loss: 0.001153599820099771, acc.: 100.0] [G loss: 7.458620548248291]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "269 [D loss: 0.0013147348654456437, acc.: 100.0] [G loss: 7.45646333694458]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "270 [D loss: 0.0006911223754286766, acc.: 100.0] [G loss: 7.476017951965332]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "271 [D loss: 0.0009490986412856728, acc.: 100.0] [G loss: 7.486026287078857]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "272 [D loss: 0.0010663071007002145, acc.: 100.0] [G loss: 7.497979640960693]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "273 [D loss: 0.0003276760871813167, acc.: 100.0] [G loss: 7.515515327453613]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "274 [D loss: 0.0014874809421598911, acc.: 100.0] [G loss: 7.53289794921875]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "275 [D loss: 0.00061565320356749, acc.: 100.0] [G loss: 7.550594806671143]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "276 [D loss: 0.0008275760628748685, acc.: 100.0] [G loss: 7.562006950378418]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "277 [D loss: 0.0007460781198460609, acc.: 100.0] [G loss: 7.581751823425293]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "278 [D loss: 0.0012406306341290474, acc.: 100.0] [G loss: 7.606109142303467]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "279 [D loss: 0.0008006917196325958, acc.: 100.0] [G loss: 7.612393856048584]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "280 [D loss: 0.0013232491328381002, acc.: 100.0] [G loss: 7.612762451171875]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "281 [D loss: 0.0006316155195236206, acc.: 100.0] [G loss: 7.626974105834961]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "282 [D loss: 0.0011955667869187891, acc.: 100.0] [G loss: 7.609428405761719]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "283 [D loss: 0.0013173960614949465, acc.: 100.0] [G loss: 7.614828109741211]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "284 [D loss: 0.000633192015811801, acc.: 100.0] [G loss: 7.617155075073242]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "285 [D loss: 0.001729034585878253, acc.: 100.0] [G loss: 7.62966775894165]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "286 [D loss: 0.0008807067351881415, acc.: 100.0] [G loss: 7.639410972595215]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "287 [D loss: 0.0008080752159003168, acc.: 100.0] [G loss: 7.647828102111816]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "288 [D loss: 0.00024202514623539173, acc.: 100.0] [G loss: 7.662789344787598]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "289 [D loss: 0.0004457917239051312, acc.: 100.0] [G loss: 7.693092346191406]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "290 [D loss: 0.0009383380529470742, acc.: 100.0] [G loss: 7.720476150512695]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "291 [D loss: 0.0007961938390508294, acc.: 100.0] [G loss: 7.748006820678711]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "292 [D loss: 0.0013104120007483289, acc.: 100.0] [G loss: 7.744853496551514]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "293 [D loss: 0.0006527008372358978, acc.: 100.0] [G loss: 7.758872032165527]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "294 [D loss: 0.0009046858613146469, acc.: 100.0] [G loss: 7.757135391235352]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "295 [D loss: 0.0010648585157468915, acc.: 100.0] [G loss: 7.766491889953613]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "296 [D loss: 0.001002963341306895, acc.: 100.0] [G loss: 7.744402885437012]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "297 [D loss: 0.001233263436006382, acc.: 100.0] [G loss: 7.735254764556885]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "298 [D loss: 0.0008418842335231602, acc.: 100.0] [G loss: 7.735236644744873]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "299 [D loss: 0.001097770087653771, acc.: 100.0] [G loss: 7.719237327575684]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "300 [D loss: 0.0009396916575497016, acc.: 100.0] [G loss: 7.703952312469482]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "301 [D loss: 0.0004847214440815151, acc.: 100.0] [G loss: 7.7018022537231445]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "302 [D loss: 0.0012762730766553432, acc.: 100.0] [G loss: 7.694073677062988]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "303 [D loss: 0.0009757274237927049, acc.: 100.0] [G loss: 7.69577169418335]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "304 [D loss: 0.0012534355628304183, acc.: 100.0] [G loss: 7.7034711837768555]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "305 [D loss: 0.0007960763032315299, acc.: 100.0] [G loss: 7.710925102233887]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "306 [D loss: 0.0006268530414672568, acc.: 100.0] [G loss: 7.725151062011719]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "307 [D loss: 0.001442321517970413, acc.: 100.0] [G loss: 7.743346691131592]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "308 [D loss: 0.0007477834878955036, acc.: 100.0] [G loss: 7.752205848693848]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "309 [D loss: 0.0011576408287510276, acc.: 100.0] [G loss: 7.7665510177612305]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "310 [D loss: 0.0005474580684676766, acc.: 100.0] [G loss: 7.778509140014648]\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "311 [D loss: 0.0011305968946544454, acc.: 100.0] [G loss: 7.791350364685059]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "312 [D loss: 0.0006120916805230081, acc.: 100.0] [G loss: 7.78757381439209]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "313 [D loss: 0.00035342329647392035, acc.: 100.0] [G loss: 7.807231426239014]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "314 [D loss: 0.0009782964625628665, acc.: 100.0] [G loss: 7.81842041015625]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "315 [D loss: 0.0009812475182116032, acc.: 100.0] [G loss: 7.841828346252441]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "316 [D loss: 0.0010431508853798732, acc.: 100.0] [G loss: 7.865062713623047]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "317 [D loss: 0.0005158111598575488, acc.: 100.0] [G loss: 7.882014274597168]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "318 [D loss: 0.001088752833311446, acc.: 100.0] [G loss: 7.893863201141357]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "319 [D loss: 0.000592623109696433, acc.: 100.0] [G loss: 7.901270866394043]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "320 [D loss: 0.0005062184645794332, acc.: 100.0] [G loss: 7.923066139221191]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "321 [D loss: 0.001126585470046848, acc.: 100.0] [G loss: 7.9223761558532715]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "322 [D loss: 0.00032228181953541934, acc.: 100.0] [G loss: 7.9293060302734375]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "323 [D loss: 0.0010165168350795284, acc.: 100.0] [G loss: 7.945232391357422]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "324 [D loss: 0.001005308164167218, acc.: 100.0] [G loss: 7.965209007263184]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "325 [D loss: 0.0008172602974809706, acc.: 100.0] [G loss: 7.982845783233643]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "326 [D loss: 0.0006740526878274977, acc.: 100.0] [G loss: 7.998652458190918]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "327 [D loss: 0.000440603937022388, acc.: 100.0] [G loss: 8.028702735900879]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "328 [D loss: 0.0009968249942176044, acc.: 100.0] [G loss: 8.047645568847656]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "329 [D loss: 0.0014873652253299952, acc.: 100.0] [G loss: 8.067434310913086]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "330 [D loss: 0.0013206799048930407, acc.: 100.0] [G loss: 8.074377059936523]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "331 [D loss: 0.0010623665002640337, acc.: 100.0] [G loss: 8.062932968139648]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "332 [D loss: 0.0007451167912222445, acc.: 100.0] [G loss: 8.049242973327637]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "333 [D loss: 0.0007640251569682732, acc.: 100.0] [G loss: 8.058256149291992]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "334 [D loss: 0.00023704424529569224, acc.: 100.0] [G loss: 8.061111450195312]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "335 [D loss: 0.0008649187802802771, acc.: 100.0] [G loss: 8.070794105529785]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "336 [D loss: 0.0008629541844129562, acc.: 100.0] [G loss: 8.084966659545898]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "337 [D loss: 0.0007839772006263956, acc.: 100.0] [G loss: 8.099610328674316]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "338 [D loss: 0.00032999452378135175, acc.: 100.0] [G loss: 8.114407539367676]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "339 [D loss: 0.0006632863223785535, acc.: 100.0] [G loss: 8.130241394042969]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "340 [D loss: 0.0008307401876663789, acc.: 100.0] [G loss: 8.14796257019043]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "341 [D loss: 0.0005314439185895026, acc.: 100.0] [G loss: 8.157374382019043]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "342 [D loss: 0.000565122434636578, acc.: 100.0] [G loss: 8.155941009521484]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "343 [D loss: 0.0011275924771325663, acc.: 100.0] [G loss: 8.154385566711426]\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "344 [D loss: 0.0008466488216072321, acc.: 100.0] [G loss: 8.164050102233887]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "345 [D loss: 0.001020706695271656, acc.: 100.0] [G loss: 8.166146278381348]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "346 [D loss: 0.000543529138667509, acc.: 100.0] [G loss: 8.162460327148438]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "347 [D loss: 0.0006912783719599247, acc.: 100.0] [G loss: 8.15704345703125]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "348 [D loss: 0.0010591211030259728, acc.: 100.0] [G loss: 8.141153335571289]\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "349 [D loss: 0.000569609081139788, acc.: 100.0] [G loss: 8.129183769226074]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "350 [D loss: 0.0005552073562284932, acc.: 100.0] [G loss: 8.11793041229248]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "351 [D loss: 0.000334228971041739, acc.: 100.0] [G loss: 8.120628356933594]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "352 [D loss: 0.0011959882103838027, acc.: 100.0] [G loss: 8.124551773071289]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "353 [D loss: 0.0004805939388461411, acc.: 100.0] [G loss: 8.12814998626709]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "354 [D loss: 0.0007470982236554846, acc.: 100.0] [G loss: 8.141773223876953]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "355 [D loss: 0.0007431931153405458, acc.: 100.0] [G loss: 8.151611328125]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "356 [D loss: 0.00035770091926679015, acc.: 100.0] [G loss: 8.173995971679688]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "357 [D loss: 0.0007268095796462148, acc.: 100.0] [G loss: 8.188911437988281]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "358 [D loss: 0.0011208008363610134, acc.: 100.0] [G loss: 8.192646980285645]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "359 [D loss: 0.000599390099523589, acc.: 100.0] [G loss: 8.187670707702637]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "360 [D loss: 0.00047159395762719214, acc.: 100.0] [G loss: 8.19293212890625]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "361 [D loss: 0.0004973783215973526, acc.: 100.0] [G loss: 8.196063995361328]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "362 [D loss: 0.00041798110760282725, acc.: 100.0] [G loss: 8.203842163085938]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "363 [D loss: 0.0004207671445328742, acc.: 100.0] [G loss: 8.210390090942383]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "364 [D loss: 0.0004346802888903767, acc.: 100.0] [G loss: 8.221855163574219]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "365 [D loss: 0.0009870872891042382, acc.: 100.0] [G loss: 8.218121528625488]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "366 [D loss: 0.0007526802946813405, acc.: 100.0] [G loss: 8.2274808883667]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "367 [D loss: 0.0007735370309092104, acc.: 100.0] [G loss: 8.227542877197266]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "368 [D loss: 0.0009070668311323971, acc.: 100.0] [G loss: 8.22807502746582]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "369 [D loss: 0.00021966496569802985, acc.: 100.0] [G loss: 8.235413551330566]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "370 [D loss: 0.00046652689343318343, acc.: 100.0] [G loss: 8.235376358032227]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "371 [D loss: 0.0006860741559648886, acc.: 100.0] [G loss: 8.244115829467773]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "372 [D loss: 0.0015994592831702903, acc.: 100.0] [G loss: 8.252273559570312]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "373 [D loss: 0.0006603335204999894, acc.: 100.0] [G loss: 8.252471923828125]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "374 [D loss: 0.0006115708674769849, acc.: 100.0] [G loss: 8.261760711669922]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "375 [D loss: 0.00033954788523260504, acc.: 100.0] [G loss: 8.274768829345703]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "376 [D loss: 0.00032384942460339516, acc.: 100.0] [G loss: 8.283880233764648]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "377 [D loss: 0.0006451666995417327, acc.: 100.0] [G loss: 8.295756340026855]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "378 [D loss: 0.0003051026869798079, acc.: 100.0] [G loss: 8.306406021118164]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "379 [D loss: 0.0002680430334294215, acc.: 100.0] [G loss: 8.316940307617188]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "380 [D loss: 0.0009059686562977731, acc.: 100.0] [G loss: 8.337204933166504]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "381 [D loss: 0.0008138937773765065, acc.: 100.0] [G loss: 8.349759101867676]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "382 [D loss: 0.0005227743313298561, acc.: 100.0] [G loss: 8.358534812927246]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "383 [D loss: 0.0008380200451938435, acc.: 100.0] [G loss: 8.35798454284668]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "384 [D loss: 0.000535270752152428, acc.: 100.0] [G loss: 8.357099533081055]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "385 [D loss: 0.0002926533416029997, acc.: 100.0] [G loss: 8.358636856079102]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "386 [D loss: 0.0004564308692351915, acc.: 100.0] [G loss: 8.358102798461914]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "387 [D loss: 0.000577837330638431, acc.: 100.0] [G loss: 8.359182357788086]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "388 [D loss: 0.0007804762208252214, acc.: 100.0] [G loss: 8.355427742004395]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "389 [D loss: 0.0005777746409876272, acc.: 100.0] [G loss: 8.345788955688477]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "390 [D loss: 0.0008015169587451965, acc.: 100.0] [G loss: 8.340363502502441]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "391 [D loss: 0.0005827891945955344, acc.: 100.0] [G loss: 8.33856201171875]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "392 [D loss: 0.00022450892720371485, acc.: 100.0] [G loss: 8.344748497009277]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "393 [D loss: 0.0006299366214079782, acc.: 100.0] [G loss: 8.350919723510742]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "394 [D loss: 0.000630882364930585, acc.: 100.0] [G loss: 8.366291046142578]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "395 [D loss: 0.0005392376260715537, acc.: 100.0] [G loss: 8.371166229248047]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "396 [D loss: 0.0007017246898612939, acc.: 100.0] [G loss: 8.380271911621094]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "397 [D loss: 0.000609595408604946, acc.: 100.0] [G loss: 8.37403678894043]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "398 [D loss: 0.0007100829461705871, acc.: 100.0] [G loss: 8.379789352416992]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "399 [D loss: 0.0006495607376564294, acc.: 100.0] [G loss: 8.392669677734375]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "400 [D loss: 0.00039776605262886733, acc.: 100.0] [G loss: 8.404830932617188]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "401 [D loss: 0.0005634181143250316, acc.: 100.0] [G loss: 8.408943176269531]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "402 [D loss: 0.0004806536016985774, acc.: 100.0] [G loss: 8.409032821655273]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "403 [D loss: 0.0005414943443611264, acc.: 100.0] [G loss: 8.406688690185547]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "404 [D loss: 0.00031565487006446347, acc.: 100.0] [G loss: 8.410725593566895]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "405 [D loss: 0.0007777774007990956, acc.: 100.0] [G loss: 8.412969589233398]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "406 [D loss: 0.00028274927171878517, acc.: 100.0] [G loss: 8.414764404296875]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "407 [D loss: 0.000536413470399566, acc.: 100.0] [G loss: 8.409934043884277]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "408 [D loss: 0.0006328160234261304, acc.: 100.0] [G loss: 8.416786193847656]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "409 [D loss: 0.0003269211374572478, acc.: 100.0] [G loss: 8.416893005371094]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "410 [D loss: 0.0001566824721521698, acc.: 100.0] [G loss: 8.433042526245117]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "411 [D loss: 0.0008074633733485825, acc.: 100.0] [G loss: 8.432205200195312]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "412 [D loss: 0.0002908077367465012, acc.: 100.0] [G loss: 8.44479751586914]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "413 [D loss: 0.00030568688816856593, acc.: 100.0] [G loss: 8.459419250488281]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "414 [D loss: 0.00031066771771293133, acc.: 100.0] [G loss: 8.478828430175781]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "415 [D loss: 0.00038569324533455074, acc.: 100.0] [G loss: 8.484871864318848]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "416 [D loss: 0.00027258651971351355, acc.: 100.0] [G loss: 8.502103805541992]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "417 [D loss: 0.00029860747599741444, acc.: 100.0] [G loss: 8.516525268554688]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "418 [D loss: 0.000377131684217602, acc.: 100.0] [G loss: 8.52460765838623]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "419 [D loss: 0.000677704272675328, acc.: 100.0] [G loss: 8.531883239746094]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "420 [D loss: 0.0005256308868410997, acc.: 100.0] [G loss: 8.530824661254883]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "421 [D loss: 0.00044151109614176676, acc.: 100.0] [G loss: 8.541114807128906]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "422 [D loss: 0.00044999773672316223, acc.: 100.0] [G loss: 8.547750473022461]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "423 [D loss: 0.00042406501597724855, acc.: 100.0] [G loss: 8.563481330871582]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "424 [D loss: 0.00043258899677312, acc.: 100.0] [G loss: 8.574324607849121]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "425 [D loss: 0.00037989395787008107, acc.: 100.0] [G loss: 8.582680702209473]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "426 [D loss: 0.0003871909939334728, acc.: 100.0] [G loss: 8.589028358459473]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "427 [D loss: 0.00036824675771640614, acc.: 100.0] [G loss: 8.593365669250488]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "428 [D loss: 0.0004842561174882576, acc.: 100.0] [G loss: 8.590076446533203]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "429 [D loss: 0.0003041249656234868, acc.: 100.0] [G loss: 8.593043327331543]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "430 [D loss: 0.00028131827275501564, acc.: 100.0] [G loss: 8.595552444458008]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "431 [D loss: 0.000575423204281833, acc.: 100.0] [G loss: 8.595996856689453]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "432 [D loss: 0.0006132988346507773, acc.: 100.0] [G loss: 8.594085693359375]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "433 [D loss: 0.00038985672290436924, acc.: 100.0] [G loss: 8.592514991760254]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "434 [D loss: 0.0002138351192115806, acc.: 100.0] [G loss: 8.602834701538086]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "435 [D loss: 0.0005958816545899026, acc.: 100.0] [G loss: 8.598703384399414]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "436 [D loss: 0.0004262800212018192, acc.: 100.0] [G loss: 8.596606254577637]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "437 [D loss: 0.0005456063008750789, acc.: 100.0] [G loss: 8.588252067565918]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "438 [D loss: 0.0002683319471543655, acc.: 100.0] [G loss: 8.587974548339844]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "439 [D loss: 0.0003866205661324784, acc.: 100.0] [G loss: 8.580605506896973]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "440 [D loss: 0.00042511205538176, acc.: 100.0] [G loss: 8.584897994995117]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "441 [D loss: 0.0004301123844925314, acc.: 100.0] [G loss: 8.592159271240234]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "442 [D loss: 0.0005452022596728057, acc.: 100.0] [G loss: 8.602795600891113]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "443 [D loss: 0.00023941168910823762, acc.: 100.0] [G loss: 8.614784240722656]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "444 [D loss: 0.0002970796631416306, acc.: 100.0] [G loss: 8.62984848022461]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "445 [D loss: 0.00027773746114689857, acc.: 100.0] [G loss: 8.642236709594727]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "446 [D loss: 0.0008070627227425575, acc.: 100.0] [G loss: 8.65762710571289]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "447 [D loss: 0.0005382510717026889, acc.: 100.0] [G loss: 8.649271011352539]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "448 [D loss: 0.0007519142091041431, acc.: 100.0] [G loss: 8.654176712036133]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "449 [D loss: 0.000441324504208751, acc.: 100.0] [G loss: 8.658477783203125]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "450 [D loss: 0.00034272226912435144, acc.: 100.0] [G loss: 8.663705825805664]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "451 [D loss: 0.0005581160949077457, acc.: 100.0] [G loss: 8.665228843688965]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "452 [D loss: 0.00033921332214958966, acc.: 100.0] [G loss: 8.667652130126953]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "453 [D loss: 0.00046307063894346356, acc.: 100.0] [G loss: 8.674476623535156]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "454 [D loss: 0.0003680500667542219, acc.: 100.0] [G loss: 8.679929733276367]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "455 [D loss: 0.00028616316558327526, acc.: 100.0] [G loss: 8.68768310546875]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "456 [D loss: 0.00043956286390312016, acc.: 100.0] [G loss: 8.701801300048828]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "457 [D loss: 0.00038931373273953795, acc.: 100.0] [G loss: 8.710355758666992]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "458 [D loss: 0.0003866654515150003, acc.: 100.0] [G loss: 8.718867301940918]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "459 [D loss: 0.00036125910992268473, acc.: 100.0] [G loss: 8.7328519821167]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "460 [D loss: 0.0003067653306061402, acc.: 100.0] [G loss: 8.743839263916016]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "461 [D loss: 0.0005013767586206086, acc.: 100.0] [G loss: 8.748750686645508]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "462 [D loss: 0.0003502898325677961, acc.: 100.0] [G loss: 8.755760192871094]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "463 [D loss: 0.0005691348414984532, acc.: 100.0] [G loss: 8.76315689086914]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "464 [D loss: 0.00025062940403586254, acc.: 100.0] [G loss: 8.771760940551758]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "465 [D loss: 0.0005222964246058837, acc.: 100.0] [G loss: 8.781137466430664]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "466 [D loss: 0.0001258121628779918, acc.: 100.0] [G loss: 8.789134979248047]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "467 [D loss: 0.0004004337970400229, acc.: 100.0] [G loss: 8.795146942138672]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "468 [D loss: 0.0005071902269264683, acc.: 100.0] [G loss: 8.799299240112305]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "469 [D loss: 0.0005225515196798369, acc.: 100.0] [G loss: 8.802753448486328]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "470 [D loss: 0.0003464818437350914, acc.: 100.0] [G loss: 8.812116622924805]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "471 [D loss: 0.0003908604849129915, acc.: 100.0] [G loss: 8.812568664550781]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "472 [D loss: 0.0004755361587740481, acc.: 100.0] [G loss: 8.809062004089355]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "473 [D loss: 0.00022114755847724155, acc.: 100.0] [G loss: 8.810752868652344]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "474 [D loss: 0.00027834944194182754, acc.: 100.0] [G loss: 8.814590454101562]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "475 [D loss: 0.0005290824774419889, acc.: 100.0] [G loss: 8.822603225708008]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "476 [D loss: 0.0003463709363131784, acc.: 100.0] [G loss: 8.821781158447266]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "477 [D loss: 0.000310013085254468, acc.: 100.0] [G loss: 8.823409080505371]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "478 [D loss: 0.0004975222254870459, acc.: 100.0] [G loss: 8.82512092590332]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "479 [D loss: 0.00039211758121382445, acc.: 100.0] [G loss: 8.827900886535645]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "480 [D loss: 0.00027703886735253036, acc.: 100.0] [G loss: 8.827291488647461]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "481 [D loss: 0.0005003959959140047, acc.: 100.0] [G loss: 8.835741996765137]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "482 [D loss: 0.0002653553310665302, acc.: 100.0] [G loss: 8.846273422241211]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "483 [D loss: 0.0004407681408338249, acc.: 100.0] [G loss: 8.854366302490234]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "484 [D loss: 0.0002651177783263847, acc.: 100.0] [G loss: 8.863622665405273]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "485 [D loss: 0.00032048533466877416, acc.: 100.0] [G loss: 8.872185707092285]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "486 [D loss: 0.000266668685071636, acc.: 100.0] [G loss: 8.879630088806152]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "487 [D loss: 0.00013072126603219658, acc.: 100.0] [G loss: 8.887248992919922]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "488 [D loss: 0.00033435443765483797, acc.: 100.0] [G loss: 8.8990478515625]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "489 [D loss: 0.00030568483634851873, acc.: 100.0] [G loss: 8.913015365600586]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "490 [D loss: 0.000318690093990881, acc.: 100.0] [G loss: 8.919717788696289]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "491 [D loss: 0.00046121376362862065, acc.: 100.0] [G loss: 8.926087379455566]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "492 [D loss: 0.0005803623280371539, acc.: 100.0] [G loss: 8.927542686462402]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "493 [D loss: 0.00036615946009987965, acc.: 100.0] [G loss: 8.928205490112305]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "494 [D loss: 0.0005884069541934878, acc.: 100.0] [G loss: 8.928445816040039]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "495 [D loss: 0.0003405531751923263, acc.: 100.0] [G loss: 8.930377960205078]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "496 [D loss: 0.000227824566536583, acc.: 100.0] [G loss: 8.93497371673584]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "497 [D loss: 0.00026030440494650975, acc.: 100.0] [G loss: 8.941776275634766]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "498 [D loss: 0.00023244842304848135, acc.: 100.0] [G loss: 8.953792572021484]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "499 [D loss: 0.0005977394030196592, acc.: 100.0] [G loss: 8.963313102722168]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "500 [D loss: 0.0005608842766378075, acc.: 100.0] [G loss: 8.963166236877441]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "501 [D loss: 0.0004944751708535478, acc.: 100.0] [G loss: 8.95802116394043]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "502 [D loss: 0.000497415428981185, acc.: 100.0] [G loss: 8.957769393920898]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "503 [D loss: 0.00035756970464717597, acc.: 100.0] [G loss: 8.963537216186523]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "504 [D loss: 0.0005928562022745609, acc.: 100.0] [G loss: 8.962007522583008]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "505 [D loss: 0.0007371111423708498, acc.: 100.0] [G loss: 8.964099884033203]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "506 [D loss: 0.0004666181848733686, acc.: 100.0] [G loss: 8.970565795898438]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "507 [D loss: 0.00020841735386056826, acc.: 100.0] [G loss: 8.983948707580566]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "508 [D loss: 0.00023575904924655333, acc.: 100.0] [G loss: 8.991849899291992]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "509 [D loss: 0.00047832562995608896, acc.: 100.0] [G loss: 9.004988670349121]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "510 [D loss: 0.00045412407780531794, acc.: 100.0] [G loss: 9.016138076782227]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "511 [D loss: 0.0005291241868690122, acc.: 100.0] [G loss: 9.014608383178711]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "512 [D loss: 0.0003119538087048568, acc.: 100.0] [G loss: 9.019224166870117]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "513 [D loss: 0.000618196529103443, acc.: 100.0] [G loss: 9.017902374267578]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "514 [D loss: 0.00022827893917565234, acc.: 100.0] [G loss: 9.016040802001953]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "515 [D loss: 0.00038405690793297254, acc.: 100.0] [G loss: 9.0167818069458]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "516 [D loss: 0.00025870126410154626, acc.: 100.0] [G loss: 9.022924423217773]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "517 [D loss: 0.00032079266384243965, acc.: 100.0] [G loss: 9.027010917663574]\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "518 [D loss: 0.0003297399052826222, acc.: 100.0] [G loss: 9.034589767456055]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "519 [D loss: 0.00022200702733243816, acc.: 100.0] [G loss: 9.042649269104004]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "520 [D loss: 0.0004676751050283201, acc.: 100.0] [G loss: 9.053996086120605]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "521 [D loss: 0.0004443561992957257, acc.: 100.0] [G loss: 9.067804336547852]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "522 [D loss: 0.00036258083855500445, acc.: 100.0] [G loss: 9.082273483276367]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "523 [D loss: 0.00012795101065421477, acc.: 100.0] [G loss: 9.099312782287598]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "524 [D loss: 0.00037631060331477784, acc.: 100.0] [G loss: 9.11400032043457]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "525 [D loss: 0.0002343520573049318, acc.: 100.0] [G loss: 9.131452560424805]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "526 [D loss: 0.0005159730862942524, acc.: 100.0] [G loss: 9.14572525024414]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "527 [D loss: 0.0002667361877684016, acc.: 100.0] [G loss: 9.159170150756836]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "528 [D loss: 0.0002819151159201283, acc.: 100.0] [G loss: 9.173683166503906]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "529 [D loss: 0.0004794696797034703, acc.: 100.0] [G loss: 9.173428535461426]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "530 [D loss: 0.00021539346926147118, acc.: 100.0] [G loss: 9.176359176635742]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "531 [D loss: 0.00037477968726307154, acc.: 100.0] [G loss: 9.17926025390625]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "532 [D loss: 0.0004986493950127624, acc.: 100.0] [G loss: 9.18271255493164]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "533 [D loss: 0.00034221529494971037, acc.: 100.0] [G loss: 9.179993629455566]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "534 [D loss: 0.0003560046825441532, acc.: 100.0] [G loss: 9.18121337890625]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "535 [D loss: 0.00023311594122787938, acc.: 100.0] [G loss: 9.18094253540039]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "536 [D loss: 0.0001827624364523217, acc.: 100.0] [G loss: 9.180320739746094]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "537 [D loss: 0.000273148383712396, acc.: 100.0] [G loss: 9.18328857421875]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "538 [D loss: 0.00013615874559036456, acc.: 100.0] [G loss: 9.187172889709473]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "539 [D loss: 0.00017485087300883606, acc.: 100.0] [G loss: 9.190330505371094]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "540 [D loss: 0.00039700017441646196, acc.: 100.0] [G loss: 9.198200225830078]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "541 [D loss: 0.0003797669523919467, acc.: 100.0] [G loss: 9.195792198181152]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "542 [D loss: 0.00020669640798587352, acc.: 100.0] [G loss: 9.191781997680664]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "543 [D loss: 0.00047265487228287384, acc.: 100.0] [G loss: 9.185484886169434]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "544 [D loss: 0.0003169639458064921, acc.: 100.0] [G loss: 9.1856689453125]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "545 [D loss: 0.0005839346704306081, acc.: 100.0] [G loss: 9.183736801147461]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "546 [D loss: 0.0001778782498149667, acc.: 100.0] [G loss: 9.184795379638672]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "547 [D loss: 0.0005167906201677397, acc.: 100.0] [G loss: 9.184822082519531]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "548 [D loss: 0.00031803709498490207, acc.: 100.0] [G loss: 9.182747840881348]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "549 [D loss: 0.00017476521315984428, acc.: 100.0] [G loss: 9.183137893676758]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "550 [D loss: 0.00023221173978527077, acc.: 100.0] [G loss: 9.184307098388672]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "551 [D loss: 0.00029922080284450203, acc.: 100.0] [G loss: 9.186070442199707]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "552 [D loss: 0.0004783567310369108, acc.: 100.0] [G loss: 9.183467864990234]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "553 [D loss: 0.00024106127602863126, acc.: 100.0] [G loss: 9.175577163696289]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "554 [D loss: 0.0002472967389621772, acc.: 100.0] [G loss: 9.170177459716797]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "555 [D loss: 0.0001263548110728152, acc.: 100.0] [G loss: 9.170665740966797]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "556 [D loss: 0.00034387414780212566, acc.: 100.0] [G loss: 9.175933837890625]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "557 [D loss: 0.0003226188928238116, acc.: 100.0] [G loss: 9.176565170288086]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "558 [D loss: 0.00032321454636985436, acc.: 100.0] [G loss: 9.179998397827148]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "559 [D loss: 0.0002217270666733384, acc.: 100.0] [G loss: 9.179695129394531]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "560 [D loss: 0.00023826221149647608, acc.: 100.0] [G loss: 9.18260383605957]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "561 [D loss: 0.0002657485783856828, acc.: 100.0] [G loss: 9.189203262329102]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "562 [D loss: 0.0003272591893619392, acc.: 100.0] [G loss: 9.197002410888672]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "563 [D loss: 0.0005210855524637736, acc.: 100.0] [G loss: 9.208545684814453]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "564 [D loss: 0.0001902620897453744, acc.: 100.0] [G loss: 9.2203950881958]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "565 [D loss: 0.00027946494810748845, acc.: 100.0] [G loss: 9.23194694519043]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "566 [D loss: 0.0002850573255273048, acc.: 100.0] [G loss: 9.243269920349121]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "567 [D loss: 0.00020478415171965025, acc.: 100.0] [G loss: 9.251482963562012]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "568 [D loss: 0.0003711798053700477, acc.: 100.0] [G loss: 9.261289596557617]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "569 [D loss: 0.00014766109597985633, acc.: 100.0] [G loss: 9.26939868927002]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "570 [D loss: 0.00025661328618298285, acc.: 100.0] [G loss: 9.280158996582031]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "571 [D loss: 0.0003305881255073473, acc.: 100.0] [G loss: 9.289170265197754]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "572 [D loss: 0.0003235677140764892, acc.: 100.0] [G loss: 9.296761512756348]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "573 [D loss: 0.0001118736618082039, acc.: 100.0] [G loss: 9.303771018981934]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "574 [D loss: 0.0002442422482999973, acc.: 100.0] [G loss: 9.308109283447266]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "575 [D loss: 0.0002718726100283675, acc.: 100.0] [G loss: 9.31204891204834]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "576 [D loss: 0.00011185061885043979, acc.: 100.0] [G loss: 9.31806755065918]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "577 [D loss: 0.0002536836691433564, acc.: 100.0] [G loss: 9.318923950195312]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "578 [D loss: 0.00039636856672586873, acc.: 100.0] [G loss: 9.320019721984863]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "579 [D loss: 0.00023924245033413172, acc.: 100.0] [G loss: 9.317792892456055]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "580 [D loss: 0.0002654985728440806, acc.: 100.0] [G loss: 9.311216354370117]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "581 [D loss: 0.00029949931922601536, acc.: 100.0] [G loss: 9.306846618652344]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "582 [D loss: 0.00023485720521421172, acc.: 100.0] [G loss: 9.308826446533203]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "583 [D loss: 0.00035588903847383335, acc.: 100.0] [G loss: 9.298328399658203]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "584 [D loss: 0.00020762314306921326, acc.: 100.0] [G loss: 9.291906356811523]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "585 [D loss: 0.0002949192639789544, acc.: 100.0] [G loss: 9.286669731140137]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "586 [D loss: 0.00025243985874112695, acc.: 100.0] [G loss: 9.284195899963379]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "587 [D loss: 0.0002970586356241256, acc.: 100.0] [G loss: 9.285237312316895]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "588 [D loss: 0.00024864474835339934, acc.: 100.0] [G loss: 9.288884162902832]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "589 [D loss: 0.0002532328071538359, acc.: 100.0] [G loss: 9.290613174438477]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "590 [D loss: 0.00016704396693967283, acc.: 100.0] [G loss: 9.297494888305664]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "591 [D loss: 0.00036445649311644956, acc.: 100.0] [G loss: 9.300605773925781]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "592 [D loss: 0.0002537885848141741, acc.: 100.0] [G loss: 9.305545806884766]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "593 [D loss: 7.492541226383764e-05, acc.: 100.0] [G loss: 9.3130464553833]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "594 [D loss: 0.00013820700041833334, acc.: 100.0] [G loss: 9.31862735748291]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "595 [D loss: 0.00015915347103145905, acc.: 100.0] [G loss: 9.328022003173828]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "596 [D loss: 0.00036424040445126593, acc.: 100.0] [G loss: 9.337111473083496]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "597 [D loss: 0.0003123945207335055, acc.: 100.0] [G loss: 9.34816837310791]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "598 [D loss: 0.00034938940007123165, acc.: 100.0] [G loss: 9.358728408813477]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "599 [D loss: 0.0004937866469845176, acc.: 100.0] [G loss: 9.366582870483398]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "600 [D loss: 0.00029906567942816764, acc.: 100.0] [G loss: 9.372884750366211]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "601 [D loss: 0.00030948769563110545, acc.: 100.0] [G loss: 9.378192901611328]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "602 [D loss: 0.0002615184712340124, acc.: 100.0] [G loss: 9.381885528564453]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "603 [D loss: 0.0001946048578247428, acc.: 100.0] [G loss: 9.388020515441895]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "604 [D loss: 0.0002245469222543761, acc.: 100.0] [G loss: 9.39194107055664]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "605 [D loss: 0.00019093076116405427, acc.: 100.0] [G loss: 9.3958740234375]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "606 [D loss: 0.0002908288333856035, acc.: 100.0] [G loss: 9.400398254394531]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "607 [D loss: 0.00023880488879512995, acc.: 100.0] [G loss: 9.405946731567383]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "608 [D loss: 0.000284904039290268, acc.: 100.0] [G loss: 9.403284072875977]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "609 [D loss: 0.0002426500286674127, acc.: 100.0] [G loss: 9.399609565734863]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "610 [D loss: 0.0004165830905549228, acc.: 100.0] [G loss: 9.396896362304688]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "611 [D loss: 0.00018772633848129772, acc.: 100.0] [G loss: 9.395614624023438]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "612 [D loss: 0.00017591407595318742, acc.: 100.0] [G loss: 9.394976615905762]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "613 [D loss: 0.00042861051406362094, acc.: 100.0] [G loss: 9.39126968383789]\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "614 [D loss: 0.00024412100174231455, acc.: 100.0] [G loss: 9.391202926635742]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "615 [D loss: 0.00033676121529424563, acc.: 100.0] [G loss: 9.392148971557617]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "616 [D loss: 0.00025498408649582416, acc.: 100.0] [G loss: 9.39208984375]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "617 [D loss: 0.00021290292352205142, acc.: 100.0] [G loss: 9.392539024353027]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "618 [D loss: 0.00019815179985016584, acc.: 100.0] [G loss: 9.396657943725586]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "619 [D loss: 0.00020429895084816962, acc.: 100.0] [G loss: 9.403579711914062]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "620 [D loss: 0.00024332682733074762, acc.: 100.0] [G loss: 9.411596298217773]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "621 [D loss: 0.00011332906797179021, acc.: 100.0] [G loss: 9.420034408569336]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "622 [D loss: 0.0002378609569859691, acc.: 100.0] [G loss: 9.42703628540039]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "623 [D loss: 0.00032517508225282654, acc.: 100.0] [G loss: 9.42929458618164]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "624 [D loss: 0.00011296000593574718, acc.: 100.0] [G loss: 9.430717468261719]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "625 [D loss: 0.0001628004538360983, acc.: 100.0] [G loss: 9.430632591247559]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "626 [D loss: 0.00014605125033995137, acc.: 100.0] [G loss: 9.433662414550781]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "627 [D loss: 0.00031339586712419987, acc.: 100.0] [G loss: 9.433258056640625]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "628 [D loss: 0.00027968629729002714, acc.: 100.0] [G loss: 9.43362808227539]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "629 [D loss: 0.0003282245379523374, acc.: 100.0] [G loss: 9.433426856994629]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "630 [D loss: 0.00036051400820724666, acc.: 100.0] [G loss: 9.428489685058594]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "631 [D loss: 0.0002009688105317764, acc.: 100.0] [G loss: 9.428075790405273]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "632 [D loss: 0.0002104941595462151, acc.: 100.0] [G loss: 9.425460815429688]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "633 [D loss: 0.0002689287066459656, acc.: 100.0] [G loss: 9.425085067749023]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "634 [D loss: 0.0001622733470867388, acc.: 100.0] [G loss: 9.427057266235352]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "635 [D loss: 0.0001324224031122867, acc.: 100.0] [G loss: 9.43106460571289]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "636 [D loss: 0.00022811646704212762, acc.: 100.0] [G loss: 9.4317626953125]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "637 [D loss: 0.00017638156714383513, acc.: 100.0] [G loss: 9.43344497680664]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "638 [D loss: 0.00026243367756251246, acc.: 100.0] [G loss: 9.439809799194336]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "639 [D loss: 0.00028086741804145277, acc.: 100.0] [G loss: 9.445905685424805]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "640 [D loss: 0.0002698922544368543, acc.: 100.0] [G loss: 9.451997756958008]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "641 [D loss: 0.0002585226538940333, acc.: 100.0] [G loss: 9.455633163452148]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "642 [D loss: 0.00014096387894824147, acc.: 100.0] [G loss: 9.458895683288574]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "643 [D loss: 0.00014016441127751023, acc.: 100.0] [G loss: 9.465085983276367]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "644 [D loss: 0.0002128988526237663, acc.: 100.0] [G loss: 9.473350524902344]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "645 [D loss: 0.00017180041322717443, acc.: 100.0] [G loss: 9.479744911193848]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "646 [D loss: 0.00031277410744223744, acc.: 100.0] [G loss: 9.481754302978516]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "647 [D loss: 0.00030438780231634155, acc.: 100.0] [G loss: 9.48878002166748]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "648 [D loss: 0.0002903418426285498, acc.: 100.0] [G loss: 9.4930419921875]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "649 [D loss: 0.00018286776321474463, acc.: 100.0] [G loss: 9.496210098266602]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "650 [D loss: 0.00018678020569495857, acc.: 100.0] [G loss: 9.500152587890625]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "651 [D loss: 0.00017574038065504283, acc.: 100.0] [G loss: 9.505167961120605]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "652 [D loss: 0.0001094460385502316, acc.: 100.0] [G loss: 9.50810432434082]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "653 [D loss: 0.00024404570285696536, acc.: 100.0] [G loss: 9.51341724395752]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "654 [D loss: 0.00026489078300073743, acc.: 100.0] [G loss: 9.520408630371094]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "655 [D loss: 0.00016709618648746982, acc.: 100.0] [G loss: 9.526655197143555]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "656 [D loss: 0.00027663497894536704, acc.: 100.0] [G loss: 9.532529830932617]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "657 [D loss: 0.00026889939181273803, acc.: 100.0] [G loss: 9.533952713012695]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "658 [D loss: 0.00015501542293350212, acc.: 100.0] [G loss: 9.538034439086914]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "659 [D loss: 0.00019849740783683956, acc.: 100.0] [G loss: 9.544909477233887]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "660 [D loss: 0.0001443041255697608, acc.: 100.0] [G loss: 9.54861831665039]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "661 [D loss: 0.00019011584663530812, acc.: 100.0] [G loss: 9.552663803100586]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "662 [D loss: 5.148985474079382e-05, acc.: 100.0] [G loss: 9.558197021484375]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "663 [D loss: 0.000262937926891027, acc.: 100.0] [G loss: 9.559175491333008]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "664 [D loss: 0.00011923500642296858, acc.: 100.0] [G loss: 9.560291290283203]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "665 [D loss: 0.00022522451035911217, acc.: 100.0] [G loss: 9.557798385620117]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "666 [D loss: 0.00020968532044207677, acc.: 100.0] [G loss: 9.551117897033691]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "667 [D loss: 0.0002732335342443548, acc.: 100.0] [G loss: 9.54696273803711]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "668 [D loss: 0.0002170708539779298, acc.: 100.0] [G loss: 9.547510147094727]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "669 [D loss: 0.00012066920317010954, acc.: 100.0] [G loss: 9.550460815429688]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "670 [D loss: 0.00013791895617032424, acc.: 100.0] [G loss: 9.555974960327148]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "671 [D loss: 0.00025922665372490883, acc.: 100.0] [G loss: 9.563957214355469]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "672 [D loss: 0.00011081020420533605, acc.: 100.0] [G loss: 9.571331024169922]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "673 [D loss: 0.0003271520290581975, acc.: 100.0] [G loss: 9.577981948852539]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "674 [D loss: 0.0002650524584169034, acc.: 100.0] [G loss: 9.586886405944824]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "675 [D loss: 0.00022206432913662866, acc.: 100.0] [G loss: 9.596719741821289]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "676 [D loss: 0.00020928632147843018, acc.: 100.0] [G loss: 9.603978157043457]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "677 [D loss: 0.0001281049735553097, acc.: 100.0] [G loss: 9.609766006469727]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "678 [D loss: 0.00027837285233545117, acc.: 100.0] [G loss: 9.61258602142334]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "679 [D loss: 0.0001917607369250618, acc.: 100.0] [G loss: 9.618485450744629]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "680 [D loss: 8.457885633106343e-05, acc.: 100.0] [G loss: 9.626054763793945]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "681 [D loss: 0.00016848137602210045, acc.: 100.0] [G loss: 9.630167961120605]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "682 [D loss: 0.00020940669492119923, acc.: 100.0] [G loss: 9.629024505615234]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "683 [D loss: 0.00016823069017846137, acc.: 100.0] [G loss: 9.62879467010498]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "684 [D loss: 0.00040023385372478515, acc.: 100.0] [G loss: 9.622946739196777]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "685 [D loss: 0.00021338000806281343, acc.: 100.0] [G loss: 9.621062278747559]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "686 [D loss: 0.00032538217419642024, acc.: 100.0] [G loss: 9.618146896362305]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "687 [D loss: 7.334839392569847e-05, acc.: 100.0] [G loss: 9.618629455566406]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "688 [D loss: 0.00023347578826360404, acc.: 100.0] [G loss: 9.615362167358398]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "689 [D loss: 0.00010548541104071774, acc.: 100.0] [G loss: 9.612886428833008]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "690 [D loss: 0.00019752805383177474, acc.: 100.0] [G loss: 9.610027313232422]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "691 [D loss: 0.00017349970221403055, acc.: 100.0] [G loss: 9.608429908752441]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "692 [D loss: 7.270708738360554e-05, acc.: 100.0] [G loss: 9.610109329223633]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "693 [D loss: 0.0002195096603827551, acc.: 100.0] [G loss: 9.6121826171875]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "694 [D loss: 0.00012489789878600277, acc.: 100.0] [G loss: 9.615432739257812]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "695 [D loss: 0.0003700356464833021, acc.: 100.0] [G loss: 9.618417739868164]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "696 [D loss: 0.00015928857828839682, acc.: 100.0] [G loss: 9.62237548828125]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "697 [D loss: 0.00013911109999753535, acc.: 100.0] [G loss: 9.623579978942871]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "698 [D loss: 0.00021323550754459575, acc.: 100.0] [G loss: 9.623777389526367]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "699 [D loss: 0.00015302599422284402, acc.: 100.0] [G loss: 9.622262954711914]\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "700 [D loss: 7.40244795451872e-05, acc.: 100.0] [G loss: 9.623945236206055]\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "701 [D loss: 0.0001273648776987102, acc.: 100.0] [G loss: 9.62623405456543]\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "702 [D loss: 0.00010899040353251621, acc.: 100.0] [G loss: 9.630874633789062]\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "703 [D loss: 0.00011811968215624802, acc.: 100.0] [G loss: 9.638030052185059]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "704 [D loss: 0.00021413378999568522, acc.: 100.0] [G loss: 9.639955520629883]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "705 [D loss: 0.00021034492237959057, acc.: 100.0] [G loss: 9.640164375305176]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "706 [D loss: 0.00011809770148829557, acc.: 100.0] [G loss: 9.639044761657715]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "707 [D loss: 0.00032716570422053337, acc.: 100.0] [G loss: 9.635209083557129]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "708 [D loss: 0.0001517988130217418, acc.: 100.0] [G loss: 9.632672309875488]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "709 [D loss: 0.00012624801456695423, acc.: 100.0] [G loss: 9.633740425109863]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "710 [D loss: 0.00018115070997737348, acc.: 100.0] [G loss: 9.633681297302246]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "711 [D loss: 0.00018965539493365213, acc.: 100.0] [G loss: 9.634361267089844]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "712 [D loss: 0.00022128169803181663, acc.: 100.0] [G loss: 9.632064819335938]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "713 [D loss: 0.0002645379208843224, acc.: 100.0] [G loss: 9.633456230163574]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "714 [D loss: 0.00016141051673912443, acc.: 100.0] [G loss: 9.635868072509766]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "715 [D loss: 0.00011904175698873587, acc.: 100.0] [G loss: 9.63852310180664]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "716 [D loss: 0.00019787957353400998, acc.: 100.0] [G loss: 9.63951301574707]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "717 [D loss: 0.00011303113933536224, acc.: 100.0] [G loss: 9.64419937133789]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "718 [D loss: 0.00015368193271569908, acc.: 100.0] [G loss: 9.64804458618164]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "719 [D loss: 0.0001703423295111861, acc.: 100.0] [G loss: 9.65457534790039]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "720 [D loss: 0.0002008732953981962, acc.: 100.0] [G loss: 9.660970687866211]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "721 [D loss: 9.42279257287737e-05, acc.: 100.0] [G loss: 9.667033195495605]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "722 [D loss: 0.00018204115986009128, acc.: 100.0] [G loss: 9.67243766784668]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "723 [D loss: 5.128790871822275e-05, acc.: 100.0] [G loss: 9.678473472595215]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "724 [D loss: 0.00012157012315583415, acc.: 100.0] [G loss: 9.688569068908691]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "725 [D loss: 0.00023440784570993856, acc.: 100.0] [G loss: 9.694972038269043]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "726 [D loss: 0.00017538735846756026, acc.: 100.0] [G loss: 9.700838088989258]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "727 [D loss: 8.491872176819015e-05, acc.: 100.0] [G loss: 9.706072807312012]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "728 [D loss: 0.0001566902246850077, acc.: 100.0] [G loss: 9.708986282348633]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "729 [D loss: 6.480561205535196e-05, acc.: 100.0] [G loss: 9.711464881896973]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "730 [D loss: 0.00018900135910371318, acc.: 100.0] [G loss: 9.712610244750977]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "731 [D loss: 9.584130020812154e-05, acc.: 100.0] [G loss: 9.712175369262695]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "732 [D loss: 0.00018585179714136757, acc.: 100.0] [G loss: 9.719291687011719]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "733 [D loss: 0.0002555172141001094, acc.: 100.0] [G loss: 9.718579292297363]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "734 [D loss: 0.0002692925718292827, acc.: 100.0] [G loss: 9.719583511352539]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "735 [D loss: 0.00029623607406392694, acc.: 100.0] [G loss: 9.714284896850586]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "736 [D loss: 0.0003062933174078353, acc.: 100.0] [G loss: 9.703689575195312]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "737 [D loss: 0.00017762069182936102, acc.: 100.0] [G loss: 9.699897766113281]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "738 [D loss: 0.0001549048611195758, acc.: 100.0] [G loss: 9.694849014282227]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "739 [D loss: 0.00010774698603199795, acc.: 100.0] [G loss: 9.690709114074707]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "740 [D loss: 0.00020821164071094245, acc.: 100.0] [G loss: 9.686012268066406]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "741 [D loss: 0.00010962041415041313, acc.: 100.0] [G loss: 9.683561325073242]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "742 [D loss: 7.120516238501295e-05, acc.: 100.0] [G loss: 9.67967700958252]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "743 [D loss: 0.0001120433080359362, acc.: 100.0] [G loss: 9.67711067199707]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "744 [D loss: 0.00012115337449358776, acc.: 100.0] [G loss: 9.659727096557617]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "745 [D loss: 0.0001567987201269716, acc.: 100.0] [G loss: 9.652087211608887]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "746 [D loss: 0.0002511767888790928, acc.: 100.0] [G loss: 9.598468780517578]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "747 [D loss: 0.00011036243449780159, acc.: 100.0] [G loss: 9.582576751708984]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "748 [D loss: 0.00015704615361755714, acc.: 100.0] [G loss: 9.508642196655273]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "749 [D loss: 0.00014661799650639296, acc.: 100.0] [G loss: 9.44631290435791]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "750 [D loss: 0.00017897208090289496, acc.: 100.0] [G loss: 9.342663764953613]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "751 [D loss: 0.00012882256123702973, acc.: 100.0] [G loss: 9.112069129943848]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "752 [D loss: 0.0003078637382714078, acc.: 100.0] [G loss: 8.828320503234863]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "753 [D loss: 0.0001804801395337563, acc.: 100.0] [G loss: 8.140130996704102]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "754 [D loss: 0.0005678143643308431, acc.: 100.0] [G loss: 7.310976505279541]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "755 [D loss: 0.0019677334639709443, acc.: 100.0] [G loss: 5.584444999694824]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "756 [D loss: 0.02807532364386134, acc.: 100.0] [G loss: 3.9077773094177246]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "757 [D loss: 0.17448714522470254, acc.: 96.875] [G loss: 4.206471920013428]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "758 [D loss: 0.06826118640310597, acc.: 100.0] [G loss: 5.282630920410156]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "759 [D loss: 0.02891688811359927, acc.: 100.0] [G loss: 4.9298295974731445]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "760 [D loss: 0.1048253932967782, acc.: 100.0] [G loss: 4.72747802734375]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "761 [D loss: 0.05770663358271122, acc.: 100.0] [G loss: 5.284414768218994]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "762 [D loss: 0.14891772251576185, acc.: 96.875] [G loss: 6.426680564880371]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "763 [D loss: 0.13672995835077018, acc.: 96.875] [G loss: 7.297389507293701]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "764 [D loss: 0.13831443979870528, acc.: 96.875] [G loss: 7.259228229522705]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "765 [D loss: 0.10848397831432521, acc.: 96.875] [G loss: 6.381457328796387]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "766 [D loss: 0.13656034576706588, acc.: 93.75] [G loss: 4.779263496398926]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "767 [D loss: 0.008327136412844993, acc.: 100.0] [G loss: 3.769047737121582]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "768 [D loss: 0.02976208459585905, acc.: 100.0] [G loss: 3.2491466999053955]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "769 [D loss: 0.036818727850914, acc.: 100.0] [G loss: 3.2029078006744385]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "770 [D loss: 0.022776668425649405, acc.: 100.0] [G loss: 3.6505160331726074]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "771 [D loss: 0.009799872976145707, acc.: 100.0] [G loss: 4.3658647537231445]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "772 [D loss: 0.016801759600639343, acc.: 100.0] [G loss: 5.012880325317383]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "773 [D loss: 0.007326739374548197, acc.: 100.0] [G loss: 5.5896782875061035]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "774 [D loss: 0.006134638097137213, acc.: 100.0] [G loss: 6.05591344833374]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "775 [D loss: 0.005939391441643238, acc.: 100.0] [G loss: 6.423235893249512]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "776 [D loss: 0.023121763253584504, acc.: 100.0] [G loss: 6.490606307983398]\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "777 [D loss: 0.007976915803737938, acc.: 100.0] [G loss: 6.478324890136719]\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "778 [D loss: 0.0064683398231863976, acc.: 100.0] [G loss: 6.425739288330078]\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "779 [D loss: 0.003212027600966394, acc.: 100.0] [G loss: 6.3770647048950195]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "780 [D loss: 0.00113990661338903, acc.: 100.0] [G loss: 6.36186408996582]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "781 [D loss: 0.001120531145716086, acc.: 100.0] [G loss: 6.373654842376709]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "782 [D loss: 0.0023374101147055626, acc.: 100.0] [G loss: 6.392590045928955]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "783 [D loss: 0.0033102152519859374, acc.: 100.0] [G loss: 6.40563440322876]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "784 [D loss: 0.0019619709928520024, acc.: 100.0] [G loss: 6.428459644317627]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "785 [D loss: 0.0028767447220161557, acc.: 100.0] [G loss: 6.448727607727051]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "786 [D loss: 0.0036899185506626964, acc.: 100.0] [G loss: 6.455878734588623]\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "787 [D loss: 0.000799101897428045, acc.: 100.0] [G loss: 6.486207008361816]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "788 [D loss: 0.002445527003146708, acc.: 100.0] [G loss: 6.517611503601074]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "789 [D loss: 0.0034020774764940143, acc.: 100.0] [G loss: 6.53841495513916]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "790 [D loss: 0.002336431643925607, acc.: 100.0] [G loss: 6.559201240539551]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "791 [D loss: 0.0017583983135409653, acc.: 100.0] [G loss: 6.591994285583496]\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "792 [D loss: 0.0021471534273587167, acc.: 100.0] [G loss: 6.62476921081543]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "793 [D loss: 0.00215719873085618, acc.: 100.0] [G loss: 6.659222602844238]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "794 [D loss: 0.0013632485643029213, acc.: 100.0] [G loss: 6.700769901275635]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "795 [D loss: 0.001933834922965616, acc.: 100.0] [G loss: 6.740947246551514]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "796 [D loss: 0.0005865161529072793, acc.: 100.0] [G loss: 6.79493522644043]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "797 [D loss: 0.0013664208818227053, acc.: 100.0] [G loss: 6.851170063018799]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "798 [D loss: 0.0017787490505725145, acc.: 100.0] [G loss: 6.899414539337158]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "799 [D loss: 0.0012262051459401846, acc.: 100.0] [G loss: 6.950966835021973]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "800 [D loss: 0.0011262257758062333, acc.: 100.0] [G loss: 7.001922607421875]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "801 [D loss: 0.0004615098296198994, acc.: 100.0] [G loss: 7.061757564544678]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "802 [D loss: 0.0017013767210301012, acc.: 100.0] [G loss: 7.107992172241211]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "803 [D loss: 0.0011742921196855605, acc.: 100.0] [G loss: 7.155394554138184]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "804 [D loss: 0.0009613034781068563, acc.: 100.0] [G loss: 7.199650287628174]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "805 [D loss: 0.0016217302181757987, acc.: 100.0] [G loss: 7.235864639282227]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "806 [D loss: 0.0025608405703678727, acc.: 100.0] [G loss: 7.249513149261475]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "807 [D loss: 0.001496360288001597, acc.: 100.0] [G loss: 7.261274337768555]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "808 [D loss: 0.0004747231141664088, acc.: 100.0] [G loss: 7.281677722930908]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "809 [D loss: 0.0008686950022820383, acc.: 100.0] [G loss: 7.305780410766602]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "810 [D loss: 0.0010210405453108251, acc.: 100.0] [G loss: 7.3318891525268555]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "811 [D loss: 0.0008846316486597061, acc.: 100.0] [G loss: 7.357839107513428]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "812 [D loss: 0.00037774888915009797, acc.: 100.0] [G loss: 7.3908843994140625]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "813 [D loss: 0.000488024961668998, acc.: 100.0] [G loss: 7.429959774017334]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "814 [D loss: 0.001376225205603987, acc.: 100.0] [G loss: 7.459767818450928]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "815 [D loss: 0.00035245189064880833, acc.: 100.0] [G loss: 7.495509624481201]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "816 [D loss: 0.00028921982266183477, acc.: 100.0] [G loss: 7.534388065338135]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "817 [D loss: 0.000751119339838624, acc.: 100.0] [G loss: 7.57071590423584]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "818 [D loss: 0.00032243884925264865, acc.: 100.0] [G loss: 7.609877586364746]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "819 [D loss: 0.0007265784952323884, acc.: 100.0] [G loss: 7.643917083740234]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "820 [D loss: 0.0002520992093195673, acc.: 100.0] [G loss: 7.682436943054199]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "821 [D loss: 0.0007150281744543463, acc.: 100.0] [G loss: 7.715334892272949]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "822 [D loss: 0.0012018226989312097, acc.: 100.0] [G loss: 7.737153053283691]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "823 [D loss: 0.000811185032944195, acc.: 100.0] [G loss: 7.758556365966797]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "824 [D loss: 0.000747357786167413, acc.: 100.0] [G loss: 7.776573181152344]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "825 [D loss: 0.0009576189622748643, acc.: 100.0] [G loss: 7.790630340576172]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "826 [D loss: 0.00027570882230065763, acc.: 100.0] [G loss: 7.812687397003174]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "827 [D loss: 0.0003184887900715694, acc.: 100.0] [G loss: 7.836800575256348]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "828 [D loss: 0.0007245783926919103, acc.: 100.0] [G loss: 7.859216690063477]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "829 [D loss: 0.0007114242180250585, acc.: 100.0] [G loss: 7.876620292663574]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "830 [D loss: 0.0007559071527794003, acc.: 100.0] [G loss: 7.893009185791016]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "831 [D loss: 0.0007486305403290316, acc.: 100.0] [G loss: 7.908806800842285]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "832 [D loss: 0.0010964429093291983, acc.: 100.0] [G loss: 7.913809776306152]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "833 [D loss: 0.0002443200719426386, acc.: 100.0] [G loss: 7.926989555358887]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "834 [D loss: 0.0011192136444151402, acc.: 100.0] [G loss: 7.931446075439453]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "835 [D loss: 0.00024025568563956767, acc.: 100.0] [G loss: 7.942519664764404]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "836 [D loss: 0.000705630169250071, acc.: 100.0] [G loss: 7.9540300369262695]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "837 [D loss: 0.0011298199970042333, acc.: 100.0] [G loss: 7.956913948059082]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "838 [D loss: 0.00028159124485682696, acc.: 100.0] [G loss: 7.9666619300842285]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "839 [D loss: 0.0002749901468632743, acc.: 100.0] [G loss: 7.981258869171143]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "840 [D loss: 0.00022336510301101953, acc.: 100.0] [G loss: 7.999570369720459]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "841 [D loss: 0.0005865563871338964, acc.: 100.0] [G loss: 8.016973495483398]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "842 [D loss: 0.0005736131861340255, acc.: 100.0] [G loss: 8.031340599060059]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "843 [D loss: 0.0009796118101803586, acc.: 100.0] [G loss: 8.039201736450195]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "844 [D loss: 0.0006565925141330808, acc.: 100.0] [G loss: 8.044632911682129]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "845 [D loss: 0.0002577622508397326, acc.: 100.0] [G loss: 8.056922912597656]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "846 [D loss: 0.000598557002376765, acc.: 100.0] [G loss: 8.067670822143555]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "847 [D loss: 0.0007226398447528481, acc.: 100.0] [G loss: 8.077356338500977]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "848 [D loss: 0.0006360088591463864, acc.: 100.0] [G loss: 8.086487770080566]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "849 [D loss: 0.0005473072524182498, acc.: 100.0] [G loss: 8.094826698303223]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "850 [D loss: 0.00020156673781457357, acc.: 100.0] [G loss: 8.106352806091309]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "851 [D loss: 0.0009012345690280199, acc.: 100.0] [G loss: 8.11377239227295]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "852 [D loss: 0.0002023166889557615, acc.: 100.0] [G loss: 8.125389099121094]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "853 [D loss: 0.00019261839770479128, acc.: 100.0] [G loss: 8.139652252197266]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "854 [D loss: 0.00020526282241917215, acc.: 100.0] [G loss: 8.158230781555176]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "855 [D loss: 0.000561674460186623, acc.: 100.0] [G loss: 8.173295974731445]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "856 [D loss: 0.0005916601803619415, acc.: 100.0] [G loss: 8.186319351196289]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "857 [D loss: 0.00022584701218875125, acc.: 100.0] [G loss: 8.204947471618652]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "858 [D loss: 0.0009007243497762829, acc.: 100.0] [G loss: 8.212421417236328]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "859 [D loss: 0.0009018983691930771, acc.: 100.0] [G loss: 8.215761184692383]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "860 [D loss: 0.0005804506363347173, acc.: 100.0] [G loss: 8.21864128112793]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "861 [D loss: 0.0004967535787727684, acc.: 100.0] [G loss: 8.221826553344727]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "862 [D loss: 0.00025526268291287124, acc.: 100.0] [G loss: 8.22914981842041]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "863 [D loss: 0.0005584012251347303, acc.: 100.0] [G loss: 8.23651123046875]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "864 [D loss: 0.0005894274363527074, acc.: 100.0] [G loss: 8.243227005004883]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "865 [D loss: 0.00021323379769455642, acc.: 100.0] [G loss: 8.254130363464355]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "866 [D loss: 0.0004732704837806523, acc.: 100.0] [G loss: 8.263850212097168]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "867 [D loss: 0.0007962881500134245, acc.: 100.0] [G loss: 8.267251968383789]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "868 [D loss: 0.0005430316086858511, acc.: 100.0] [G loss: 8.271344184875488]\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "869 [D loss: 0.0008209377847379074, acc.: 100.0] [G loss: 8.270795822143555]\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "870 [D loss: 0.0004896353784715757, acc.: 100.0] [G loss: 8.271407127380371]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "871 [D loss: 0.00016932795188040473, acc.: 100.0] [G loss: 8.27723503112793]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "872 [D loss: 0.0008301490743178874, acc.: 100.0] [G loss: 8.278263092041016]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "873 [D loss: 0.0011926862789550796, acc.: 100.0] [G loss: 8.270883560180664]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "874 [D loss: 0.0002121504003298469, acc.: 100.0] [G loss: 8.27064323425293]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "875 [D loss: 0.0004963144310750067, acc.: 100.0] [G loss: 8.271012306213379]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "876 [D loss: 0.0004978488432243466, acc.: 100.0] [G loss: 8.274625778198242]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "877 [D loss: 0.0004611447948263958, acc.: 100.0] [G loss: 8.276575088500977]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "878 [D loss: 0.0007403066847473383, acc.: 100.0] [G loss: 8.275192260742188]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "879 [D loss: 0.0007931078725960106, acc.: 100.0] [G loss: 8.272515296936035]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "880 [D loss: 0.00047534370969515294, acc.: 100.0] [G loss: 8.272842407226562]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "881 [D loss: 0.0005065140285296366, acc.: 100.0] [G loss: 8.272940635681152]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "882 [D loss: 0.00012958742900082143, acc.: 100.0] [G loss: 8.280403137207031]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "883 [D loss: 0.0006641230429522693, acc.: 100.0] [G loss: 8.283929824829102]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "884 [D loss: 0.00013703909189644037, acc.: 100.0] [G loss: 8.291940689086914]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "885 [D loss: 0.0007422131311614066, acc.: 100.0] [G loss: 8.296905517578125]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "886 [D loss: 0.0004179016686975956, acc.: 100.0] [G loss: 8.303033828735352]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "887 [D loss: 0.000224862655159086, acc.: 100.0] [G loss: 8.31419849395752]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "888 [D loss: 0.0003863295423798263, acc.: 100.0] [G loss: 8.325271606445312]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "889 [D loss: 0.0001951845915755257, acc.: 100.0] [G loss: 8.338153839111328]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "890 [D loss: 0.00015865703971940093, acc.: 100.0] [G loss: 8.355277061462402]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "891 [D loss: 0.00046611316793132573, acc.: 100.0] [G loss: 8.371182441711426]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "892 [D loss: 0.0006204747623996809, acc.: 100.0] [G loss: 8.382200241088867]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "893 [D loss: 0.00039707291580270976, acc.: 100.0] [G loss: 8.391867637634277]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "894 [D loss: 0.000151896685565589, acc.: 100.0] [G loss: 8.406017303466797]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "895 [D loss: 0.0008608457283116877, acc.: 100.0] [G loss: 8.4105224609375]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "896 [D loss: 0.00038852999568916857, acc.: 100.0] [G loss: 8.416701316833496]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "897 [D loss: 0.00012022828832414234, acc.: 100.0] [G loss: 8.426412582397461]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "898 [D loss: 0.0006554035353474319, acc.: 100.0] [G loss: 8.43220329284668]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "899 [D loss: 0.0004095132098882459, acc.: 100.0] [G loss: 8.438102722167969]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "900 [D loss: 0.00018077122513204813, acc.: 100.0] [G loss: 8.447761535644531]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "901 [D loss: 0.0001459160921513103, acc.: 100.0] [G loss: 8.461734771728516]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "902 [D loss: 0.00016715880337869748, acc.: 100.0] [G loss: 8.477760314941406]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "903 [D loss: 0.00040351647476200014, acc.: 100.0] [G loss: 8.491666793823242]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "904 [D loss: 0.0005821498343721032, acc.: 100.0] [G loss: 8.501420974731445]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "905 [D loss: 0.00044845553929917514, acc.: 100.0] [G loss: 8.509322166442871]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "906 [D loss: 0.0005930047336732969, acc.: 100.0] [G loss: 8.513984680175781]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "907 [D loss: 0.00019312398217152804, acc.: 100.0] [G loss: 8.523115158081055]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "908 [D loss: 0.000560378612135537, acc.: 100.0] [G loss: 8.528402328491211]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "909 [D loss: 0.00035958451917394996, acc.: 100.0] [G loss: 8.533563613891602]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "910 [D loss: 0.0001298572460655123, acc.: 100.0] [G loss: 8.542094230651855]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "911 [D loss: 0.0001294934336328879, acc.: 100.0] [G loss: 8.555171966552734]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "912 [D loss: 0.000571290627704002, acc.: 100.0] [G loss: 8.562345504760742]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "913 [D loss: 0.00013287601177580655, acc.: 100.0] [G loss: 8.572973251342773]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "914 [D loss: 0.00034460105234757066, acc.: 100.0] [G loss: 8.583754539489746]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "915 [D loss: 0.0008097082900349051, acc.: 100.0] [G loss: 8.58645248413086]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "916 [D loss: 0.00013002563355257735, acc.: 100.0] [G loss: 8.592336654663086]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "917 [D loss: 0.0003647079720394686, acc.: 100.0] [G loss: 8.599451065063477]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "918 [D loss: 0.0009736688152770512, acc.: 100.0] [G loss: 8.594415664672852]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "919 [D loss: 0.0013570267474278808, acc.: 100.0] [G loss: 8.57653522491455]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "920 [D loss: 0.0001849653126555495, acc.: 100.0] [G loss: 8.566679000854492]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "921 [D loss: 0.0005530021080630831, acc.: 100.0] [G loss: 8.555879592895508]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "922 [D loss: 0.0005204139451961964, acc.: 100.0] [G loss: 8.545509338378906]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "923 [D loss: 0.0005131576326675713, acc.: 100.0] [G loss: 8.535988807678223]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "924 [D loss: 0.000506049495015759, acc.: 100.0] [G loss: 8.528019905090332]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "925 [D loss: 0.0001035269524436444, acc.: 100.0] [G loss: 8.52564525604248]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "926 [D loss: 0.00010185498013015604, acc.: 100.0] [G loss: 8.529850006103516]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "927 [D loss: 0.000490532569529023, acc.: 100.0] [G loss: 8.532176971435547]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "928 [D loss: 0.0001268701089429669, acc.: 100.0] [G loss: 8.538965225219727]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "929 [D loss: 0.0003052916727028787, acc.: 100.0] [G loss: 8.54787826538086]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "930 [D loss: 0.00032599997939541936, acc.: 100.0] [G loss: 8.556547164916992]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "931 [D loss: 0.00033143260225187987, acc.: 100.0] [G loss: 8.565580368041992]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "932 [D loss: 0.0004752299137180671, acc.: 100.0] [G loss: 8.572750091552734]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "933 [D loss: 0.0006252938619581982, acc.: 100.0] [G loss: 8.574651718139648]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "934 [D loss: 0.0006226035766303539, acc.: 100.0] [G loss: 8.57310962677002]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "935 [D loss: 0.00048342479567509145, acc.: 100.0] [G loss: 8.57115364074707]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "936 [D loss: 0.00014497295342152938, acc.: 100.0] [G loss: 8.574226379394531]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "937 [D loss: 0.0003098515953752212, acc.: 100.0] [G loss: 8.577959060668945]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "938 [D loss: 0.00012241184958838858, acc.: 100.0] [G loss: 8.587259292602539]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "939 [D loss: 0.00028789701900677755, acc.: 100.0] [G loss: 8.597867965698242]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "940 [D loss: 0.00010308591663488187, acc.: 100.0] [G loss: 8.610639572143555]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "941 [D loss: 0.0002673599519766867, acc.: 100.0] [G loss: 8.622702598571777]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "942 [D loss: 0.00014186626503942534, acc.: 100.0] [G loss: 8.638028144836426]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "943 [D loss: 0.00025389751681359485, acc.: 100.0] [G loss: 8.652627944946289]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "944 [D loss: 0.00044655444798991084, acc.: 100.0] [G loss: 8.665129661560059]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "945 [D loss: 0.00042588022188283503, acc.: 100.0] [G loss: 8.673446655273438]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "946 [D loss: 0.00025611137971282005, acc.: 100.0] [G loss: 8.681303024291992]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "947 [D loss: 0.0001098884913517395, acc.: 100.0] [G loss: 8.694950103759766]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "948 [D loss: 0.0001560961754876189, acc.: 100.0] [G loss: 8.708178520202637]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "949 [D loss: 0.000567676717764698, acc.: 100.0] [G loss: 8.715130805969238]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "950 [D loss: 0.000266743685642723, acc.: 100.0] [G loss: 8.723846435546875]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "951 [D loss: 0.00024067811318673193, acc.: 100.0] [G loss: 8.73263168334961]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "952 [D loss: 0.0004429268592502922, acc.: 100.0] [G loss: 8.738956451416016]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "953 [D loss: 0.00010614425627863966, acc.: 100.0] [G loss: 8.747127532958984]\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "954 [D loss: 0.0010129017682629637, acc.: 100.0] [G loss: 8.744683265686035]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "955 [D loss: 0.00026393806183477864, acc.: 100.0] [G loss: 8.743167877197266]\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "956 [D loss: 0.0006879965658299625, acc.: 100.0] [G loss: 8.73757266998291]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "957 [D loss: 0.000550595999811776, acc.: 100.0] [G loss: 8.728127479553223]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "958 [D loss: 0.00042432571353856474, acc.: 100.0] [G loss: 8.720294952392578]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "959 [D loss: 0.00025232488405890763, acc.: 100.0] [G loss: 8.716882705688477]\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "960 [D loss: 0.0003733418561751023, acc.: 100.0] [G loss: 8.71241569519043]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "961 [D loss: 0.00015129824168980122, acc.: 100.0] [G loss: 8.714508056640625]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "962 [D loss: 0.00040267957956530154, acc.: 100.0] [G loss: 8.715529441833496]\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "963 [D loss: 0.00040307926246896386, acc.: 100.0] [G loss: 8.71722412109375]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "964 [D loss: 0.0002816366541082971, acc.: 100.0] [G loss: 8.720442771911621]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "965 [D loss: 0.0005299856275087222, acc.: 100.0] [G loss: 8.721078872680664]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "966 [D loss: 0.0004845483636017889, acc.: 100.0] [G loss: 8.718918800354004]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "967 [D loss: 0.00022457593877334148, acc.: 100.0] [G loss: 8.719619750976562]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "968 [D loss: 0.00022000629542162642, acc.: 100.0] [G loss: 8.7239351272583]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "969 [D loss: 0.0003815485251834616, acc.: 100.0] [G loss: 8.725902557373047]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "970 [D loss: 0.0001077116685337387, acc.: 100.0] [G loss: 8.732887268066406]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "971 [D loss: 0.00023590730415889993, acc.: 100.0] [G loss: 8.741937637329102]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "972 [D loss: 0.00022974959574639797, acc.: 100.0] [G loss: 8.751081466674805]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "973 [D loss: 0.00020795483578694984, acc.: 100.0] [G loss: 8.762628555297852]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "974 [D loss: 0.00037889285886194557, acc.: 100.0] [G loss: 8.769437789916992]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "975 [D loss: 8.948274444264825e-05, acc.: 100.0] [G loss: 8.780593872070312]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "976 [D loss: 0.00010380497406003997, acc.: 100.0] [G loss: 8.795299530029297]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "977 [D loss: 0.00020190249051665887, acc.: 100.0] [G loss: 8.808395385742188]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "978 [D loss: 0.00039958675915841013, acc.: 100.0] [G loss: 8.818811416625977]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "979 [D loss: 0.0001175339930341579, acc.: 100.0] [G loss: 8.832865715026855]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "980 [D loss: 0.0002211796963820234, acc.: 100.0] [G loss: 8.845233917236328]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "981 [D loss: 0.0003975789004471153, acc.: 100.0] [G loss: 8.8560152053833]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "982 [D loss: 0.00032510194432688877, acc.: 100.0] [G loss: 8.86404800415039]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "983 [D loss: 0.00033862034615594894, acc.: 100.0] [G loss: 8.870752334594727]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "984 [D loss: 0.00031640925590181723, acc.: 100.0] [G loss: 8.875529289245605]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "985 [D loss: 0.0003368771431269124, acc.: 100.0] [G loss: 8.879928588867188]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "986 [D loss: 0.0002283161593368277, acc.: 100.0] [G loss: 8.884787559509277]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "987 [D loss: 0.00023153591610025615, acc.: 100.0] [G loss: 8.8922119140625]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "988 [D loss: 0.00020985432638553903, acc.: 100.0] [G loss: 8.899755477905273]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "989 [D loss: 9.615263479645364e-05, acc.: 100.0] [G loss: 8.908979415893555]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "990 [D loss: 0.00044763383630197495, acc.: 100.0] [G loss: 8.915583610534668]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "991 [D loss: 0.00011129676568089053, acc.: 100.0] [G loss: 8.924318313598633]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "992 [D loss: 0.0003083377378061414, acc.: 100.0] [G loss: 8.931158065795898]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "993 [D loss: 0.00045000729733146727, acc.: 100.0] [G loss: 8.93464469909668]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "994 [D loss: 0.0002050494367722422, acc.: 100.0] [G loss: 8.939702033996582]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "995 [D loss: 0.0002011499454965815, acc.: 100.0] [G loss: 8.944686889648438]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "996 [D loss: 0.0001838125754147768, acc.: 100.0] [G loss: 8.952157974243164]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "997 [D loss: 0.00021739228395745158, acc.: 100.0] [G loss: 8.958837509155273]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "998 [D loss: 8.859128320182208e-05, acc.: 100.0] [G loss: 8.968867301940918]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "999 [D loss: 0.0002923465217463672, acc.: 100.0] [G loss: 8.977280616760254]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "GAN Generated Text:  this this this this this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers"
      ],
      "metadata": {
        "id": "IYxCjjQ1TgH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install transformers"
      ],
      "metadata": {
        "id": "Y1Gy9-g0oPVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "class GPT2TextGenerator:\n",
        "    def __init__(self, model_name='gpt2'):\n",
        "        # Load pre-trained model and tokenizer\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "    def generate(self, seed_text, max_length=50):\n",
        "        # Encode input text\n",
        "        input_ids = self.tokenizer.encode(seed_text, return_tensors='pt')\n",
        "\n",
        "        # Generate text\n",
        "        output = self.model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
        "\n",
        "        # Decode output text\n",
        "        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        return generated_text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    seed_text = \"This is a sample text. It is used to train the Markov chain.\"\n",
        "\n",
        "    # Create GPT-2 text generator\n",
        "    gpt2_generator = GPT2TextGenerator()\n",
        "\n",
        "    # Generate a sentence\n",
        "    sentence = gpt2_generator.generate(seed_text, max_length=50)\n",
        "\n",
        "    # Print the generated sentence\n",
        "    print(\"GPT-2 Generated Text: \", sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvwZy-yfpvGw",
        "outputId": "e843e6e0-604a-4006-f8be-b46bd9e9ca45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 Generated Text:  This is a sample text. It is used to train the Markov chain.\n",
            "\n",
            "The Markova chain is the most common chain in the world. The Markovan chain has a very high number of users. In the past, the number was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iMyjoZWdp2zr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}